[{"categories":["爬虫"],"content":"爬虫 Scrapy高级 使用 ","date":"2017-07-09","objectID":"/posts/spider-7%E5%8A%A8%E4%BD%9C%E9%93%BE%E8%87%AA%E5%8A%A8%E7%99%BB%E5%BD%9512306/:0:0","tags":["爬虫","scrapy"],"title":"动作链","uri":"/posts/spider-7%E5%8A%A8%E4%BD%9C%E9%93%BE%E8%87%AA%E5%8A%A8%E7%99%BB%E5%BD%9512306/"},{"categories":["爬虫"],"content":"动作链 from selenium import webdriver from selenium.webdriver import ActionChains import time bro=webdriver.Chrome(executable_path='./chromedriver') bro.get('https://www.runoob.com/try/try.php?filename=jqueryui-api-droppable') bro.implicitly_wait(10) #切换frame（很少） bro.switch_to.frame('iframeResult') div=bro.find_element_by_xpath('//*[@id=\"draggable\"]') # 使用动作链 #得到一个动作练对象 action=ActionChains(bro) # 使用动作链 #点击并且夯住 action.click_and_hold(div) # 直接把上面的div移动到某个元素上 # action.move_to_element(元素控件) # 移动x坐标，y坐标 # 三种移动方式 # action.move_by_offset() # 通过坐标 # action.move_to_element() # 到另一个标签 # action.move_to_element_with_offset() # 到另一个标签，再偏移一部分 for i in range(5): action.move_by_offset(10,10) # 直接把上面的div移动到某个元素上的某个位置 # action.move_to_element_with_offset() # 调用它，会动起来 action.perform() time.sleep(1) #释放动作链 action.release() time.sleep(5) bro.close() ","date":"2017-07-09","objectID":"/posts/spider-7%E5%8A%A8%E4%BD%9C%E9%93%BE%E8%87%AA%E5%8A%A8%E7%99%BB%E5%BD%9512306/:1:0","tags":["爬虫","scrapy"],"title":"动作链","uri":"/posts/spider-7%E5%8A%A8%E4%BD%9C%E9%93%BE%E8%87%AA%E5%8A%A8%E7%99%BB%E5%BD%9512306/"},{"categories":["爬虫"],"content":"爬虫 Scrapy高级 使用 ","date":"2017-07-08","objectID":"/posts/spider-6scrapy%E9%AB%98%E7%BA%A7/:0:0","tags":["爬虫","scrapy"],"title":"Scrapy高级","uri":"/posts/spider-6scrapy%E9%AB%98%E7%BA%A7/"},{"categories":["爬虫"],"content":"scrapy 请求传参 # 1 放 ：yield Request(url,callback=self.parser_detail,meta={'item':item}) # 2 取：response.meta.get('item') # 3 指定解析函数 callback=self.解析函数. class CnblogSpider(scrapy.Spider): name = 'cnblog' # allowed_domains = ['https://www.cnblogs.com/'] start_urls = ['https://www.cnblogs.com/sitehome/p/1'] num = 0 # post_list \u003e div:nth-child(19) \u003e div def parse(self, response): article_list = response.css(\".post_item_body\") for article in article_list: item = CrawlCnblogsItem() title = article.css(\".titlelnk::text\").extract_first() url = article.css(\".titlelnk::attr(href)\").extract_first() desc = article.css(\"p::text\").extract()[-1] author = article.css(\".post_item_foot\u003ea::text\").extract_first() comment = article.xpath(\".//*[contains(@class,'post_item_foot')]/span[1]/a/text()\").extract_first() view = article.xpath(\".//*[contains(@class,'post_item_foot')]/span[2]/a/text()\").extract_first() item[\"title\"] = title.strip() item[\"url\"] = url item[\"desc\"] = desc item[\"author\"] = author.strip() item[\"comment\"] = comment.strip() item[\"view\"] = view.strip() yield item nextpage = f'https://www.cnblogs.com{response.css(\".pager\u003e a:last-child::attr(href)\").extract_first()}' if nextpage and (self.num \u003c 10): yield Request(url=nextpage, callback=self.parse) ","date":"2017-07-08","objectID":"/posts/spider-6scrapy%E9%AB%98%E7%BA%A7/:1:0","tags":["爬虫","scrapy"],"title":"Scrapy高级","uri":"/posts/spider-6scrapy%E9%AB%98%E7%BA%A7/"},{"categories":["爬虫"],"content":"提高爬取效率 - 在配置文件中进行相关的配置即可:(默认还有一套setting) #1 增加并发： 默认scrapy开启的并发线程为32个，可以适当进行增加。在settings配置文件中修改CONCURRENT_REQUESTS = 100值为100,并发设置成了为100。 #2 提高日志级别： 在运行scrapy时，会有大量日志信息的输出，为了减少CPU的使用率。可以设置log输出信息为INFO或者ERROR即可。在配置文件中编写：LOG_LEVEL = ‘INFO’ # 3 禁止cookie： 如果不是真的需要cookie，则在scrapy爬取数据时可以禁止cookie从而减少CPU的使用率，提升爬取效率。在配置文件中编写：COOKIES_ENABLED = False # 4禁止重试： 对失败的HTTP进行重新请求（重试）会减慢爬取速度，因此可以禁止重试。在配置文件中编写：RETRY_ENABLED = False # 5 减少下载超时： 如果对一个非常慢的链接进行爬取，减少下载超时可以能让卡住的链接快速被放弃，从而提升效率。在配置文件中进行编写：DOWNLOAD_TIMEOUT = 10 超时时间为10s ","date":"2017-07-08","objectID":"/posts/spider-6scrapy%E9%AB%98%E7%BA%A7/:2:0","tags":["爬虫","scrapy"],"title":"Scrapy高级","uri":"/posts/spider-6scrapy%E9%AB%98%E7%BA%A7/"},{"categories":["爬虫"],"content":"下载中间件 # 2大中间件：下载中间件，爬虫中间件 # 1 写在middlewares.py中（名字随便命名） # 2 配置生效（） SPIDER_MIDDLEWARES = { 'cnblogs_crawl.middlewares.CnblogsCrawlSpiderMiddleware': 543, } DOWNLOADER_MIDDLEWARES = { 'cnblogs_crawl.middlewares.CnblogsCrawlDownloaderMiddleware': 543, } # 2 下载中间件 -process_request：（请求去，走） # - return None: 继续处理当次请求，进入下一个中间件 # - return Response： 当次请求结束，把Response丢给引擎处理（可以自己爬，包装成Response） # - return Request ： 相当于把Request重新给了引擎，引擎再去做调度 # - 抛异常：执行process_exception -process_response：（请求回来，走） # - return a Response object ：继续处理当次Response，继续走后续的中间件 # - return a Request object：重新给引擎做调度 # - or raise IgnoreRequest ：process_exception -process_exception：（出异常，走） # - return None: continue processing this exception # - return a Response object: stops process_exception() chain ：停止异常处理链，给引擎（给爬虫） # - return a Request object: stops process_exception() chain ：停止异常处理链，给引擎（重新调度） ","date":"2017-07-08","objectID":"/posts/spider-6scrapy%E9%AB%98%E7%BA%A7/:3:0","tags":["爬虫","scrapy"],"title":"Scrapy高级","uri":"/posts/spider-6scrapy%E9%AB%98%E7%BA%A7/"},{"categories":["爬虫"],"content":"集成 selenium # 在爬虫已启动，就打开一个chrom浏览器，以后都用这一个浏览器来爬数据 # 1 在爬虫中创建bro对象 bro = webdriver.Chrome(executable_path='./chromedriver') # 2 中间件中使用： spider.bro.get(request.url) text=spider.bro.page_source response=HtmlResponse(url=request.url,status=200,body=text.encode('utf-8')) return response # 3 关闭，在爬虫中 def close(self, reason): self.bro.close() ","date":"2017-07-08","objectID":"/posts/spider-6scrapy%E9%AB%98%E7%BA%A7/:4:0","tags":["爬虫","scrapy"],"title":"Scrapy高级","uri":"/posts/spider-6scrapy%E9%AB%98%E7%BA%A7/"},{"categories":["爬虫"],"content":"fack-useragent # 请求头中的user-agent list=['',''] # pip3 install fake-useragent # https://github.com/hellysmile/fake-useragent from fake_useragent import UserAgent ua=UserAgent(verify_ssl=False) print(ua.random) ","date":"2017-07-08","objectID":"/posts/spider-6scrapy%E9%AB%98%E7%BA%A7/:5:0","tags":["爬虫","scrapy"],"title":"Scrapy高级","uri":"/posts/spider-6scrapy%E9%AB%98%E7%BA%A7/"},{"categories":["爬虫"],"content":"去重源码分析 # 去重源码分析 # from scrapy.core.scheduler import Scheduler # Scheduler下：def enqueue_request(self, request)方法判断是否去重 if not request.dont_filter and self.df.request_seen(request): Requests对象，RFPDupeFilter对象 # 如果要自己写一个去重类 -写一个类，继承BaseDupeFilter类 -重写def request_seen(self, request): -在setting中配置：DUPEFILTER_CLASS = '项目名.dup.UrlFilter' # scrapy起始爬取的地址 def start_requests(self): for url in self.start_urls: yield Request(url) 增量爬取 -增量爬取（100链接，150个链接） -已经爬过的，放到某个位置（mysql，redis中：集合） -如果用默认的，爬过的地址，放在内存中，只要项目一重启，就没了，它也不知道我爬过那个了，所以要自己重写去重方案 -你写的去重方案，占得内存空间更小 -bitmap方案 -BloomFilter布隆过滤器 from scrapy.http import Request from scrapy.utils.request import request_fingerprint # 这种网址是一个 requests1=Request(url='https://www.baidu.com?name=lqz\u0026age=19') requests2=Request(url='https://www.baidu.com?age=18\u0026name=lqz') ret1=request_fingerprint(requests1) ret2=request_fingerprint(requests2) print(ret1) print(ret2) # bitmap去重 一个小格表示一个连接地址 32个连接，一个比特位来存一个地址 # https://www.baidu.com?age=18\u0026name=lqz ---》44 # https://www.baidu.com?age=19\u0026name=lqz ---》89 # c2c73dfccf73bf175b903c82b06a31bc7831b545假设它占4个bytes，4*8=32个比特位 # 存一个地址，占32个比特位 # 10个地址，占320个比特位 #计算机计量单位 # 比特位：只能存0和1 # 8个比特位是一个bytes # 1024bytes=1kb # 1024kb=1m # 1024m=1g # 布隆过滤器：原理和python中如何使用 def request_seen(self, request): # 把request对象传入request_fingerprint得到一个值：aefasdfeasd # 把request对象，唯一生成一个字符串 fp = self.request_fingerprint(request) #判断fp，是否在集合中，在集合中，表示已经爬过，return True，他就不会再爬了 if fp in self.fingerprints: return True # 如果不在集合中，放到集合中 self.fingerprints.add(fp) if self.file: self.file.write(fp + os.linesep) ","date":"2017-07-08","objectID":"/posts/spider-6scrapy%E9%AB%98%E7%BA%A7/:6:0","tags":["爬虫","scrapy"],"title":"Scrapy高级","uri":"/posts/spider-6scrapy%E9%AB%98%E7%BA%A7/"},{"categories":["爬虫"],"content":"分布式爬虫 # 1 安装pip3 install scrapy-redis # 1 原来的爬虫继承 from scrapy_redis.spiders import RedisSpider class CnblogsSpider(RedisSpider): #start_urls = ['http://www.cnblogs.com/'] redis_key = 'myspider:start_urls' # 2 在setting中配置 SCHEDULER = \"scrapy_redis.scheduler.Scheduler\" DUPEFILTER_CLASS = \"scrapy_redis.dupefilter.RFPDupeFilter\" ITEM_PIPELINES = { 'scrapy_redis.pipelines.RedisPipeline': 300 } # 3 多台机器上启动scrapy # 4 向reids中发送起始url lpush myspider:start_urls https://www.cnblogs.com ","date":"2017-07-08","objectID":"/posts/spider-6scrapy%E9%AB%98%E7%BA%A7/:7:0","tags":["爬虫","scrapy"],"title":"Scrapy高级","uri":"/posts/spider-6scrapy%E9%AB%98%E7%BA%A7/"},{"categories":["爬虫"],"content":"爬虫 Scrapy框架 使用 ","date":"2017-07-07","objectID":"/posts/spider-5scrapy%E6%A1%86%E6%9E%B6/:0:0","tags":["爬虫","scrapy"],"title":"Scrapy框架","uri":"/posts/spider-5scrapy%E6%A1%86%E6%9E%B6/"},{"categories":["爬虫"],"content":"1 Scrapy 介绍/架构 Scrapy一个开源和协作的框架，其最初是为了页面抓取 (更确切来说, 网络抓取 )所设计的，使用它可以以快速、简单、可扩展的方式从网站中提取所需的数据。但目前Scrapy的用途十分广泛，可用于如数据挖掘、监测和自动化测试等领域，也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫。 Scrapy 是基于twisted框架开发而来，twisted是一个流行的事件驱动的python网络框架。因此Scrapy使用了一种非阻塞（又名异步）的代码来实现并发。整体架构大致如下 Components： 引擎(EGINE) 引擎负责控制系统所有组件之间的数据流，并在某些动作发生时触发事件。有关详细信息，请参见上面的数据流部分。 调度器(SCHEDULER) 用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL的优先级队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址 下载器(DOWLOADER) 用于下载网页内容, 并将网页内容返回给EGINE，下载器是建立在twisted这个高效的异步模型上的 爬虫(SPIDERS) SPIDERS是开发人员自定义的类，用来解析responses，并且提取items，或者发送新的请求 项目管道(ITEM PIPLINES) 在items被提取后负责处理它们，主要包括清理、验证、持久化（比如存到数据库）等操作 下载器中间件(Downloader Middlewares) 位于Scrapy引擎和下载器之间，主要用来处理从EGINE传到DOWLOADER的请求request，已经从DOWNLOADER传到EGINE的响应response，你可用该中间件做以下几件事 process a request just before it is sent to the Downloader (i.e. right before Scrapy sends the request to the website); change received response before passing it to a spider; send a new Request instead of passing received response to a spider; pass response to a spider without fetching a web page; silently drop some requests. 爬虫中间件(Spider Middlewares) 位于EGINE和SPIDERS之间，主要工作是处理SPIDERS的输入（即responses）和输出（即requests） 官网链接：https://docs.scrapy.org/en/latest/topics/architecture.html ","date":"2017-07-07","objectID":"/posts/spider-5scrapy%E6%A1%86%E6%9E%B6/:1:0","tags":["爬虫","scrapy"],"title":"Scrapy框架","uri":"/posts/spider-5scrapy%E6%A1%86%E6%9E%B6/"},{"categories":["爬虫"],"content":"2 Scripy框架的安装和启动 安装 #Windows平台 1、pip3 install wheel #安装后，便支持通过wheel文件安装软件，wheel文件官网：https://www.lfd.uci.edu/~gohlke/pythonlibs 3、pip3 install lxml 4、pip3 install pyopenssl 5、下载并安装pywin32：https://sourceforge.net/projects/pywin32/files/pywin32/ 6、下载twisted的wheel文件：http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted 7、执行pip3 install 下载目录\\Twisted-17.9.0-cp36-cp36m-win_amd64.whl 8、pip3 install scrapy #Linux和 mac平台 1、pip3 install scrapy 创建 scrapy startproject #创建项目 scrapy genspider #创建爬虫程序 启动 scrapy crawl 爬虫名字 # 启动爬虫 scrapy crawl 爬虫名字 --nolog # 不打印日志启动 从文件启动 from scrapy.cmdline import execute # execute(['scrapy','crawl','chouti','--nolog']) execute(['scrapy','crawl','chouti']) ","date":"2017-07-07","objectID":"/posts/spider-5scrapy%E6%A1%86%E6%9E%B6/:2:0","tags":["爬虫","scrapy"],"title":"Scrapy框架","uri":"/posts/spider-5scrapy%E6%A1%86%E6%9E%B6/"},{"categories":["爬虫"],"content":"3 配置文件和目录介绍 目录介绍 -crawl_chouti # 项目名 -crawl_chouti # 跟项目一个名，文件夹 -spiders # spiders：放着爬虫 genspider生成的爬虫，都放在这下面 -__init__.py -chouti.py # 抽屉爬虫 -cnblogs.py # cnblogs 爬虫 -items.py # 对比django中的models.py文件 ,写一个个的模型类 -middlewares.py # 中间件（爬虫中间件，下载中间件），中间件写在这 -pipelines.py # 写持久化的地方（持久化到文件，mysql，redis，mongodb） -settings.py # 配置文件 -scrapy.cfg # 不用关注，上线相关的 配置文件 # 配置文件 ROBOTSTXT_OBEY = False # 是否遵循爬虫协议，强行运行 USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36' # 请求头中的ua LOG_LEVEL='ERROR' # 这样配置，程序错误信息才会打印， #启动爬虫直接 scrapy crawl 爬虫名 就没有日志输出 # scrapy crawl 爬虫名 --nolog 爬虫文件 class ChoutiSpider(scrapy.Spider): name = 'chouti' # 爬虫名字 allowed_domains = ['https://dig.chouti.com/'] # 允许爬取的域 start_urls = ['https://dig.chouti.com/'] # 起始爬取的位置，爬虫一启动，会先向它发请求 def parse(self, response): # 解析，请求回来，自动执行parser，在这个方法中做解析 print('---------------------------',response) 数据解析 使用bs4 # 1 解析，可以使用bs4解析 from bs4 import BeautifulSoup soup=BeautifulSoup(response.text,'lxml') soup.find_all() 使用内置解析器 response.css response.xpath # 解析 # 所有用css或者xpath选择出来的都放在列表中 # 取第一个:extract_first() # 取出所有extract() # css选择器取文本和属性： response.css(\".link-title::text\") response.css(\".link-title::attr(href)\") # xpath选择器取文本和属性 response.xpath('.//a[contains(@class,\"link-title\")/text()]') response.xpath('//a[contains(@class,\"link-title\")/@href]') ","date":"2017-07-07","objectID":"/posts/spider-5scrapy%E6%A1%86%E6%9E%B6/:3:0","tags":["爬虫","scrapy"],"title":"Scrapy框架","uri":"/posts/spider-5scrapy%E6%A1%86%E6%9E%B6/"},{"categories":["爬虫"],"content":"4 数据持久化 # 方式一 -1 parser解析函数，return 列表，列表套字典 -2 scrapy crawl chouti -o aa.json (支持：('json', 'jsonlines', 'jl', 'csv', 'xml', 'marshal', 'pickle') # 方式二 pipline的方式（管道） -1 在items.py中创建模型类 -2 在爬虫中chouti.py，引入，把解析的数据放到item对象中（要用中括号） -3 yield item对象 -4 配置文件配置管道 ITEM_PIPELINES = { # 数字表示优先级（数字越小，优先级越大） 'crawl_chouti.pipelines.CrawlChoutiPipeline': 300, 'crawl_chouti.pipelines.CrawlChoutiRedisPipeline': 301， } -5 pipline.py中写持久化的类 # 在保存数据之前执行的函数 -spider_open # 在执行数据持久化之后执行的函数 -spider_close -process_item（在这写保存到哪） [toc] ","date":"2017-07-07","objectID":"/posts/spider-5scrapy%E6%A1%86%E6%9E%B6/:4:0","tags":["爬虫","scrapy"],"title":"Scrapy框架","uri":"/posts/spider-5scrapy%E6%A1%86%E6%9E%B6/"},{"categories":["爬虫"],"content":"爬虫 selenium 模块使用 ","date":"2017-07-06","objectID":"/posts/spider-4selenium/:0:0","tags":["爬虫","代理池"],"title":"selenium","uri":"/posts/spider-4selenium/"},{"categories":["爬虫"],"content":"介绍 selenium最初是一个自动化测试工具,而爬虫中使用它主要是为了解决requests无法直接执行JavaScript代码的问题 可以操作浏览器(火狐，谷歌（建议你用谷歌），ie)，模拟人的行为（人可以干啥，代码控制就可以干啥） ","date":"2017-07-06","objectID":"/posts/spider-4selenium/:1:0","tags":["爬虫","代理池"],"title":"selenium","uri":"/posts/spider-4selenium/"},{"categories":["爬虫"],"content":"selenium的简单使用 # pip3 install selenium # 1 基本使用 from selenium import webdriver # import time # # 得到 一个谷歌浏览器对象 # # 代码不能直接操作浏览器，需要有一个浏览器驱动（配套的） # # 下载谷歌浏览器驱动：http://npm.taobao.org/mirrors/chromedriver/ # # 谷歌浏览器驱动要跟谷歌版本对应 # # http://npm.taobao.org/mirrors/chromedriver/80.0.3987.106/ ：80.0.3987.149（正式版本） # # 指定一下驱动的位置（相对路径/绝对路径） # bro=webdriver.Chrome(executable_path='./chromedriver') # # bro.get(\"https://www.baidu.com\") # # # 页面内容 # # ret.text 相当于它，可以使用bs4解析数据，或者用selenium自带的解析器解析 # print(bro.page_source) # time.sleep(5) # bro.close() ","date":"2017-07-06","objectID":"/posts/spider-4selenium/:2:0","tags":["爬虫","代理池"],"title":"selenium","uri":"/posts/spider-4selenium/"},{"categories":["爬虫"],"content":"selenium的高级用法 常用方法 bro=webdriver.Chrome(executable_path='./chromedriver') bro.get(\"https://www.baidu.com\") 解析器 # 1、find_element_by_id # id找 # 2、find_element_by_link_text # a标签上的文字找 # 3、find_element_by_partial_link_text # a标签上的文字模糊 # 4、find_element_by_tag_name # 根据标签名字找 # 5、find_element_by_class_name # 根据类名字找 # 6、find_element_by_name # name='xx' 根据name属性找 # 7、find_element_by_css_selector # css选择器找 # 8、find_element_by_xpath #xpath选择器找 在输入框中输入美女（自带的解析器，查找输入框空间） # //*[@id=\"kw\"] # input_search=bro.find_element_by_xpath('//*[@id=\"kw\"]') input_search=bro.find_element_by_css_selector('#kw') 写文字 input_search.send_keys(\"美女\") 查找搜索按钮 enter=bro.find_element_by_id('su') 点击按钮 enter.click() 关闭浏览器 bro.close() 小案例 import time bro=webdriver.Chrome(executable_path='./chromedriver') bro.get(\"https://www.baidu.com\") # # 隐士等待(最多等待10s) # 只有控件没有加载出来，才会等，控件一旦加载出来，直接就取到 bro.implicitly_wait(10) submit_button=bro.find_element_by_link_text('登录') submit_button.click() user_button=bro.find_element_by_id('TANGRAM__PSP_10__footerULoginBtn') user_button.click() user_input=bro.find_element_by_id('TANGRAM__PSP_10__userName') user_input.send_keys(\"ssssss@qq.com\") pwd_input=bro.find_element_by_id('TANGRAM__PSP_10__password') pwd_input.send_keys(\"123456\") submit_input=bro.find_element_by_id('TANGRAM__PSP_10__submit') submit_input.click() time.sleep(5) bro.close() 获取 cookie # 登陆之后，拿到cookie：就可以自己搭建cookie池（requests模块发请求，携带者cookie） import time bro=webdriver.Chrome(executable_path='./chromedriver') bro.get(\"https://www.baidu.com\") print(bro.get_cookies()) bro.close() # #搭建cookie池和代理池的作用是什么？封ip ，封账号（弄一堆小号，一堆cookie） 无界面浏览器 from selenium.webdriver.chrome.options import Options chrome_options = Options() chrome_options.add_argument('window-size=1920x3000') #指定浏览器分辨率 chrome_options.add_argument('--disable-gpu') #谷歌文档提到需要加上这个属性来规避bug chrome_options.add_argument('--hide-scrollbars') #隐藏滚动条, 应对一些特殊页面 chrome_options.add_argument('blink-settings=imagesEnabled=false') #不加载图片, 提升速度 chrome_options.add_argument('--headless') #浏览器不提供可视化页面. linux下如果系统不支持可视化不加这条会启动失败 bro=webdriver.Chrome(executable_path='./chromedriver',options=chrome_options) bro.get(\"https://www.baidu.com\") print(bro.get_cookies()) bro.close 获取标签属性(重点) print(tag.get_attribute('src')) print(tag.get_attribute('href')) 获取标签文本(重点) print(tag.text) 获取标签ID，位置，名称，大小（了解） print(tag.id) print(tag.location) print(tag.tag_name) print(tag.size) 显示等待与隐式等待 # 隐士等待(最多等待10s) bro.implicitly_wait(10) # 只有控件没有加载出来，才会等，控件一旦加载出来，直接就取到 # 显示等待（每个控件，都要写等待），不要使用 元素交互操作 # 点击click，清空clear，输入文字send_keys 执行js import time bro=webdriver.Chrome(executable_path='./chromedriver') # bro.get(\"https://www.cnblogs.com\") # 执行js代码 # bro.execute_script('alert(1)') # window.scrollTo(0,document.body.scrollHeight) # 使页面滚动到最低层 bro.execute_script('window.scrollTo(0,document.body.scrollHeight)') time.sleep(5) bro.close() 模拟浏览器的前进后头 import time bro=webdriver.Chrome(executable_path='./chromedriver') bro.get(\"https://www.cnblogs.com\") time.sleep(1) bro.get(\"https://www.baidu.com\") time.sleep(1) bro.get(\"https://www.jd.com\") #退到上一个 bro.back() time.sleep(1) # 前进一下 bro.forward() time.sleep(5) bro.close() 选项卡管理 import time from selenium import webdriver # browser=webdriver.Chrome(executable_path='./chromedriver') browser.get('https://www.baidu.com') browser.execute_script('window.open()') # 本质上是执行的js代码 print(browser.window_handles) #获取所有的选项卡 browser.switch_to_window(browser.window_handles[1]) browser.get('https://www.taobao.com') time.sleep(2) browser.switch_to_window(browser.window_handles[0]) browser.get('https://www.sina.com.cn') browser.close() 异常处理 from selenium import webdriver from selenium.common.exceptions import TimeoutException,NoSuchElementException,NoSuchFrameException try: browser=webdriver.Chrome(executable_path='./chromedriver') browser.get('http://www.baidu.com') browser.find_element_by_id(\"xxx\") # except Exception as e: # print(e) finally: browser.close() ","date":"2017-07-06","objectID":"/posts/spider-4selenium/:3:0","tags":["爬虫","代理池"],"title":"selenium","uri":"/posts/spider-4selenium/"},{"categories":["爬虫"],"content":"小案例 ######## # 爬取京东商品信息 ####### from selenium import webdriver import time from selenium.webdriver.common.keys import Keys bro=webdriver.Chrome(executable_path='./chromedriver') def get_goods(bro): # find_elements_by_class_name 找所有 # find_element_by_class_name 找一个 li_list=bro.find_elements_by_class_name('gl-item') # ul_list=bro.find_elements_by_css_selector('.gl-item') for li in li_list: url=li.find_element_by_css_selector('.p-img\u003ea').get_attribute('href') url_img=li.find_element_by_css_selector('.p-img img').get_attribute(\"src\") if not url_img: url_img='https:'+li.find_element_by_css_selector('.p-img img').get_attribute(\"data-lazy-img\") price=li.find_element_by_css_selector('.p-price i').text name=li.find_element_by_css_selector('.p-name em').text commit=li.find_element_by_css_selector('.p-commit a').text print(''' 商品名字：%s 商品价格：%s 商品图片地址：%s 商品地址：%s 商品评论数：%s '''%(name,price,url,url_img,commit)) #查找下一页按钮 next=bro.find_element_by_partial_link_text('下一页') time.sleep(1) next.click() #继续抓取下一页 get_goods(bro) try: bro.get('https://www.jd.com') #隐士等待 bro.implicitly_wait(10) input_search=bro.find_element_by_id('key') input_search.send_keys(\"精品内衣\") #模拟键盘操作(模拟键盘敲回车) input_search.send_keys(Keys.ENTER) get_goods(bro) except Exception as e: print(e) finally: bro.close() [toc] ","date":"2017-07-06","objectID":"/posts/spider-4selenium/:3:1","tags":["爬虫","代理池"],"title":"selenium","uri":"/posts/spider-4selenium/"},{"categories":["爬虫"],"content":"爬虫 代理池 模块使用 ","date":"2017-07-06","objectID":"/posts/spider-3%E4%BB%A3%E7%90%86%E6%B1%A0/:0:0","tags":["爬虫","代理池"],"title":"代理池","uri":"/posts/spider-3%E4%BB%A3%E7%90%86%E6%B1%A0/"},{"categories":["爬虫"],"content":"免费代理池 # https://github.com/jhao104/proxy_pool # 收费的：提供给你一个接口，每掉一次这个接口，获得一个代理 # 免费：用爬虫爬取，免费代理，放到我的库中，flask，django搭一个服务（删除代理，自动测试代理可用性），每次发一个请求，获取一个代理 # 带你配置 # 1 下载，解压，用pycharm打开 # 2 安装依赖 pip install -r requirements.txt # 3 配置Config/setting.py: DB_TYPE = getenv('db_type', 'redis').upper() DB_HOST = getenv('db_host', '127.0.0.1') DB_PORT = getenv('db_port', 6379) DB_PASSWORD = getenv('db_password', '') # 4 本地启动redis-server # 5 可以在cli目录下通过ProxyPool.py -python proxyPool.py schedule :调度程序，他会取自动爬取免费代理 -python proxyPool.py webserver:启动api服务，把flask启动起来 ","date":"2017-07-06","objectID":"/posts/spider-3%E4%BB%A3%E7%90%86%E6%B1%A0/:1:0","tags":["爬虫","代理池"],"title":"代理池","uri":"/posts/spider-3%E4%BB%A3%E7%90%86%E6%B1%A0/"},{"categories":["爬虫"],"content":"验证码破解 # 1 简单验证码，字母，数字 # 2 高级的，选择，你好，12306选择乒乓球，滑动验证（极验） # 打码平台（自动破解验证码，需要花钱）云打码，超级鹰（12306） http://www.yundama.com/ http://www.chaojiying.com/ # 注册账号，（充钱）把demo下载下来，运行即可 ","date":"2017-07-06","objectID":"/posts/spider-3%E4%BB%A3%E7%90%86%E6%B1%A0/:2:0","tags":["爬虫","代理池"],"title":"代理池","uri":"/posts/spider-3%E4%BB%A3%E7%90%86%E6%B1%A0/"},{"categories":["爬虫"],"content":"爬取段子发送给女朋友 ##### # 1 爬取糗事百科，微信自动发送 ##### # https://www.qiushibaike.com/text/ # https://www.qiushibaike.com/text/page/1/ import requests from bs4 import BeautifulSoup ret=requests.get('https://www.qiushibaike.com/text/page/1/') # print(ret.text) ll=[] soup=BeautifulSoup(ret.text,\"lxml\") article_list=soup.find_all(name='div',id=True,class_='article') for article in article_list: content=article.find(name='div',class_='content').span.text # content=article.find(name='div',class_='content').text # content=article.find(class_='content').text # print(content) # 入库 #我们放到列表中 ll.append(content) print(ll) # 微信自动发消息 # wxpy：实现了web微信的接口 # pip3 install wxpy from wxpy import * # 实例化得到一个对象，微信机器人对象 import random bot=Bot(cache_path=True) @bot.register() # 接收从指定好友发来的消息，发送者即recv_msg.sender为指定好友girl_friend def recv_send_msg(recv_msg): print('收到的消息：',recv_msg.text) # recv_msg.text取得文本 return random.choice(ll) embed() ","date":"2017-07-06","objectID":"/posts/spider-3%E4%BB%A3%E7%90%86%E6%B1%A0/:3:0","tags":["爬虫","代理池"],"title":"代理池","uri":"/posts/spider-3%E4%BB%A3%E7%90%86%E6%B1%A0/"},{"categories":["爬虫"],"content":"爬虫 bs4 模块使用 ","date":"2017-07-04","objectID":"/posts/spider-2%E7%88%AC%E8%99%AB%E9%AB%98%E7%BA%A7/:0:0","tags":["爬虫","bs4"],"title":"爬虫入门/bs4","uri":"/posts/spider-2%E7%88%AC%E8%99%AB%E9%AB%98%E7%BA%A7/"},{"categories":["爬虫"],"content":"bs4的使用 从html或者xml中提取数据的python库，修改xml # 安装 pip3 install beautifulsoup4 # 使用 from bs4 import BeautifulSoup # 实例化得到对象，传入要解析的文本，解析器 # html.parser内置解析器，速度稍微慢一些，但是不需要装第三方模块 # lxml：速度快一些，但是需要安装 pip3 install lxml soup=BeautifulSoup(ret.text,'html.parser') # soup=BeautifulSaoup(open('a.html','r')) # find（找到的第一个） # find_all(找到的所有) 遍历文档树 from bs4 import BeautifulSoup html_doc = \"\"\" \u003chtml\u003e\u003chead\u003e\u003ctitle\u003eThe Dormouse's story\u003c/title\u003e\u003c/head\u003e \u003cbody\u003e \u003cp class=\"title\"id=\"id_p\"\u003e\u003cb\u003eThe Dormouse's story\u003c/b\u003e\u003c/p\u003e \u003cp class=\"story\"\u003eOnce upon a time there were three little sisters; and their names were \u003ca href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"\u003eElsie\u003c/a\u003e, \u003ca href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"\u003eLacie\u003c/a\u003e and \u003ca href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"\u003eTillie\u003c/a\u003e; and they lived at the bottom of a well.\u003c/p\u003e \u003cp class=\"story\"\u003e...\u003c/p\u003e \"\"\" # pip3 install lxml soup=BeautifulSoup(html_doc,'lxml') # 美化 # print(soup.prettify()) Tag 对象 from bs4.element import Tag #1、用法（通过.来查找，只能找到第一个） # head=soup.head # title=head.title # # print(head) # print(title) 获取标签名称 # p=soup.body # print(type(p)) # print(p.name) 获取标签属性 p=soup.p # 方式一 # 获取class属性,可以有多个，拿到列表 print(p['class']) print(p['id']) print(p.get('id')) # 方式二 print(p.attrs['class']) print(p.attrs.get('id')) 获取标签内容 p=soup.p print(p.text) # 所有层级都拿出来拼到一起 print(p.string) # 只有一层，才能去除 print(list(p.strings)) # 把每次都取出来，做成一个生成器 嵌套选择 title=soup.head.title print(title) 子节点 子孙节点 p1=soup.p.children # 迭代器 p2=soup.p.contents # 列表 print(list(p1)) print(p2) 父节点 祖先节点 p1=soup.p.parent # 直接父节点 p2=soup.p.parents print(p1) # print(len(list(p2))) print(list(p2)) 兄弟节点 print(soup.a.next_sibling) #下一个兄弟 print(soup.a.previous_sibling) #上一个兄弟 print(list(soup.a.next_siblings)) #下面的兄弟们=\u003e生成器对象 print(soup.a.previous_siblings) #上面的兄弟们=\u003e生成器对象 查找文档树 # 查找文档树（find，find_all），速度比遍历文档树慢 # 两个配合着使用（soup.p.find()） 五种过滤器(# 以find为例) 字符串 #1 字符串查找 引号内是字符串 p=soup.find(name='p') p=soup.find(name='body') print(p) # 查找类名是title的所有标签,class是关键字，class_ ret=soup.find_all(class_='title') href属性为http://example.com/elsie的标签 ret=soup.find_all(href='http://example.com/elsie') 找id为xx的标签 ret=soup.find_all(id='id_p') print(ret) 正则表达式 import re # reg=re.compile('^b') # ret=soup.find_all(name=reg) #找id以id开头的标签 reg=re.compile('^id') ret=soup.find_all(id=reg) print(ret) 列表 ret=soup.find_all(name=['body','b']) ret=soup.find_all(id=['id_p','link1']) ret=soup.find_all(class_=['id_p','link1']) # and 关系 ret=soup.find_all(class_='title',name='p') print(ret) True # 所有有名字的标签 ret=soup.find_all(name=True) #所有有id的标签 ret=soup.find_all(id=True) # 所有有herf属性的 ret=soup.find_all(href=True) print(ret) 方法 def has_class_but_no_id(tag): return tag.has_attr('class') and not tag.has_attr('id') print(soup.find_all(has_class_but_no_id)) 其他使用 ret=soup.find_all(attrs={'class':\"title\"}) ret=soup.find_all(attrs={'id':\"id_p1\",'class':'title'}) print(ret) 拿到标签取属性, 去text ret=soup.find_all(attrs={'id':\"id_p\",'class':'title'}) print(ret[0].text) limit(限制条数) soup.find() # 就是find_all limit=1 ret=soup.find_all(name=True,limit=2) print(len(ret)) recursive recursive=False (只找儿子)不递归查找，只找第一层 ret=soup.body.find_all(name='p',recursive=False) print(ret) ","date":"2017-07-04","objectID":"/posts/spider-2%E7%88%AC%E8%99%AB%E9%AB%98%E7%BA%A7/:0:1","tags":["爬虫","bs4"],"title":"爬虫入门/bs4","uri":"/posts/spider-2%E7%88%AC%E8%99%AB%E9%AB%98%E7%BA%A7/"},{"categories":["爬虫"],"content":"css 和 xpath 选择器 css 选择器 # 重点 # Tag对象.select(\"css选择器\") # #ID号 # .类名 # div\u003ep：儿子 和div p：子子孙孙 # 找div下最后一个a标签 div a:last-child # bs4：自己的选择器，css选择器 # lxml：css选择器，xpath选择器 # selenium：自己的选择器，css选择器，xpath选择器 # scrapy框架：自己的选择器，css选择器，xpath选择器 # #select('.article') #该模块提供了select方法来支持css,详见官网:https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html#id37 html_doc = \"\"\" \u003chtml\u003e\u003chead\u003e\u003ctitle\u003eThe Dormouse's story\u003c/title\u003e\u003c/head\u003e \u003cbody\u003e \u003cp class=\"title\"\u003e \u003cb\u003eThe Dormouse's story\u003c/b\u003e Once upon a time there were three little sisters; and their names were \u003ca href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"\u003e \u003cspan\u003eElsie\u003c/span\u003e \u003c/a\u003e \u003ca href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"\u003eLacie\u003c/a\u003e and \u003ca href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"\u003eTillie\u003c/a\u003e; \u003cdiv class='panel-1'\u003e \u003cul class='list' id='list-1'\u003e \u003cli class='element'\u003eFoo\u003c/li\u003e \u003cli class='element'\u003eBar\u003c/li\u003e \u003cli class='element'\u003eJay\u003c/li\u003e \u003c/ul\u003e \u003cul class='list list-small' id='list-2'\u003e \u003cli class='element'\u003e\u003ch1 class='yyyy'\u003eFoo\u003c/h1\u003e\u003c/li\u003e \u003cli class='element xxx'\u003eBar\u003c/li\u003e \u003cli class='element'\u003eJay\u003c/li\u003e \u003c/ul\u003e \u003c/div\u003e and they lived at the bottom of a well. \u003c/p\u003e \u003cp class=\"story\"\u003e...\u003c/p\u003e \"\"\" from bs4 import BeautifulSoup soup=BeautifulSoup(html_doc,'lxml') ### css 选择器 print(soup.p.select('.sister')) print(soup.select('.sister span')) print(soup.select('#link1')) print(soup.select('#link1 span')) print(soup.select('#list-2 .element.xxx')) print(soup.select('#list-2')[0].select('.element')) #可以一直select,但其实没必要,一条select就可以了 # xpath选择 # / 从根节点选取 /a 从根节点开始，往下找a标签（子） # //从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置 //a 从根节点开始找a标签（子子孙孙中所有a） # . 选取当前节点。 # .. 选取当前节点的父节点。 # @ 选取属性。 ######## # 2 xpath选择器 ######## # XPath 是一门在 XML 文档中查找信息的语言 # xpath选择 # / 从根节点选取 /a 从根节点开始，往下找a标签（子） # //从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置 //a 从根节点开始找a标签（子子孙孙中所有a） # 取值 /text() # 取属性 /@属性名 # //*[@id=\"auto-channel-lazyload-article\"]/ul[1] # //ul[1] # //*[@id=\"focus-1\"]/div[1]/ul/li[3]/h2 # #focus-1 \u003e div.focusimg-pic \u003e ul \u003e li:nth-child(3) \u003e h2 doc=''' \u003chtml\u003e \u003chead\u003e \u003cbase href='http://example.com/' /\u003e \u003ctitle\u003eExample website\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cdiv id='images'\u003e \u003ca href='image1.html' id=\"xxx\"\u003eName: My image 1 \u003cbr /\u003e\u003cimg src='image1_thumb.jpg' /\u003e\u003c/a\u003e \u003ch5\u003etest\u003c/h5\u003e \u003ca href='image2.html'\u003eName: My image 2 \u003cbr /\u003e\u003cimg src='image2_thumb.jpg' /\u003e\u003c/a\u003e \u003ca href='image3.html'\u003eName: My image 3 \u003cbr /\u003e\u003cimg src='image3_thumb.jpg' /\u003e\u003c/a\u003e \u003ca href='image4.html'\u003eName: My image 4 \u003cbr /\u003e\u003cimg src='image4_thumb.jpg' /\u003e\u003c/a\u003e \u003ca href='image5.html' class='li li-item' name='items'\u003eName: My image 5 \u003cbr /\u003e\u003cimg src='image5_thumb.jpg' /\u003e\u003c/a\u003e \u003ca href='image6.html' name='items'\u003e\u003cspan\u003e\u003ch5\u003etest\u003c/h5\u003e\u003c/span\u003eName: My image 6 \u003cbr /\u003e\u003cimg src='image6_thumb.jpg' /\u003e\u003c/a\u003e \u003c/div\u003e \u003c/body\u003e \u003c/html\u003e ''' from lxml import etree html=etree.HTML(doc) # 传字符串 # html=etree.parse('search.html',etree.HTMLParser()) # 文件 # 1 所有节点 # a=html.xpath('//*') # 2 指定节点（结果为列表） # a=html.xpath('//head') # 3 子节点，子孙节点 # a=html.xpath('//div/a') # a=html.xpath('//body/a') #无数据 # a=html.xpath('//body//a') # 4 父节点 # a=html.xpath('//body//a[@href=\"image1.html\"]/..') # a=html.xpath('//body//a[@href=\"image1.html\"]') # a=html.xpath('//body//a[1]/..') # 也可以这样 # a=html.xpath('//body//a[1]/parent::*') # 5 属性匹配 # a=html.xpath('//body//a[@href=\"image1.html\"]') # 6 文本获取 标签后加：/text() ********重点 # a=html.xpath('//body//a[@href=\"image1.html\"]/text()') # a=html.xpath('//body//a/text()') # 7 属性获取 标签后：/@href ********重点 # a=html.xpath('//body//a/@href') # # 注意从1 开始取（不是从0） # a=html.xpath('//body//a[3]/@href') # 8 属性多值匹配 # a 标签有多个class类，直接匹配就不可以了，需要用contains # a=html.xpath('//body//a[@class=\"li\"]') # a=html.xpath('//body//a[@href=\"image1.html\"]') # a=html.xpath('//body//a[contains(@class,\"li\")]') # a=html.xpath('//body//a[contains(@class,\"li\")]/text()') # a=html.xpath('//body//a[contains(@class,\"li\")]/@name') # 9 多属性匹配 or 和 and （了解） # a=html.xpath('//body//a[contains(@class,\"li\") or @name=\"items\"]') # a=html.xpath('//body//a[contains(@class,\"li\") and @name=\"items\"]/text()') # a=html.xpath('//body//a[contains(@class,\"li\"","date":"2017-07-04","objectID":"/posts/spider-2%E7%88%AC%E8%99%AB%E9%AB%98%E7%BA%A7/:0:2","tags":["爬虫","bs4"],"title":"爬虫入门/bs4","uri":"/posts/spider-2%E7%88%AC%E8%99%AB%E9%AB%98%E7%BA%A7/"},{"categories":null,"content":"爬虫 requests 模块使用 ","date":"2017-07-03","objectID":"/posts/spider-1%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/:0:0","tags":["爬虫","requests模块"],"title":"Request模块","uri":"/posts/spider-1%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/"},{"categories":null,"content":"爬虫介绍 # 1 本质：模拟发送http请求（requests）----》解析返回数据（re，bs4，lxml，json）---》入库（redis，mysql，mongodb） # 2 app爬虫：本质一模一样 # 3 为什么python做爬虫最好：包多，爬虫框架：scrapy：性能很高的爬虫框架，爬虫界的django，大而全（爬虫相关的东西都集成了） # 4 百度，谷歌，就是个大爬虫 在百度搜索，其实是去百度的服务器的库搜的，百度一直开着爬虫，一刻不停的在互联网上爬取，把页面存储到自己库中 # 5 全文检索：全文检索 ","date":"2017-07-03","objectID":"/posts/spider-1%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/:0:1","tags":["爬虫","requests模块"],"title":"Request模块","uri":"/posts/spider-1%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/"},{"categories":null,"content":"requests 模块 requests模块是基于urllib2 内置库的基础上封装的一个模块, 被广泛应用 安装方法 pip3 install requests 基本使用 返回值的属性 import requests # # 发送http请求 # # get,delete,post。。本质都是调用request函数 # ret=requests.get('https://www.cnblogs.com') # print(ret.status_code) # 响应状态码 # print(ret.text) # 响应体，转成了字符串 # print(ret.content) # 响应体，二进制 # ret=requests.post()\\ # ret=requests.request(\"get\",) # ret=requests.delete() ## get 请求 带参数 get请求带参数 方式一 ret = requests.get('https://www.baidu.com/', headers={ 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36', }) 方式二（建议用方式二）中文会自动转码 ret=requests.get('http://0.0.0.0:8001/',params={'name':\"美女\",'age':18}) print(ret.text) 带 headers ret = requests.get('http://0.0.0.0:8001/?name=%E7%BE%8E%E5%A5%B3', headers={ # 标志，什么东西发出的请求，浏览器信息，django框架，从哪取？（meta） 'User-Agent': 'request', # 上一个页面的地址，图片防盗链 'Referer': 'xxx' }) print(ret) # 图片防盗链：如果图片的referer不是我自己的网站，就直接禁止掉 \u003cimg src=\"https://www.lgstatic.com/lg-community-fed/community/modules/common/img/avatar_default_7225407.png\"\u003e 带 cookie # 带cookie,随机字符串(用户信息：也代表session)，不管后台用的token认证，还是session认证 # 一旦登陆了，带着cookie发送请求，表示登陆了（下单，12306买票，评论） 方式一 ret = requests.get('http://0.0.0.0:8001/?name=%E7%BE%8E%E5%A5%B3', headers={ 'cookie': 'key3=value;key2=value', }) 方式二 ret = requests.get('http://0.0.0.0:8001/?name=%E7%BE%8E%E5%A5%B3', cookies={\"islogin\":\"xxx\"}) print(ret) 发送post 请求 5 发送post请求（注册，登陆），携带数据（body） data=None, json=None # data:urlencoded编码 ret=requests.post('http://0.0.0.0:8001/',data={'name':\"lqz\",'age':18}) # json:json编码 import json data=json.dumps({'name':\"lqz\",'age':18}) ret=requests.post('http://0.0.0.0:8001/',json=data) print(ret) # 注意：编码格式是请求头中带的，所有我可以手动修改，在headers中改 session 对象 session=requests.session() # 跟requests.get/post用起来完全一样，但是它处理了cookie # 假设是一个登陆，并且成功 session.post() # 再向该网站发请求，就是登陆状态，不需要手动携带cookie session.get(\"地址\") 响应对象 print(respone.text) # 响应体转成str print(respone.content) # 响应体二进制（图片，视频） print(respone.status_code) # 响应状态码 print(respone.headers) # 响应头 print(respone.cookies) # 服务端返回的cookie print(respone.cookies.get_dict()) # 转成字典 print(respone.cookies.items()) print(respone.url) # 当次请求的地址 print(respone.history) # 如果有重定向，放到一个列表中 ret=requests.post('http://0.0.0.0:8001/') ret=requests.get('http://0.0.0.0:8001/admin') #不要误解 ret=requests.get('http://0.0.0.0:8001/user') print(ret.history) print(respone.encoding) # 编码方式 response.iter_content() # 视频，图片迭代取值 with open(\"a.mp4\",'wb') as f: for line in response.iter_content(): f.write(lin 乱码问题 # 加载回来的页面，打印出来，乱码（我们用的是utf8编码），如果网站用gbk， ret.encoding='gbk' ret=requests.get('http://0.0.0.0:8001/user') # ret.apparent_encoding当前页面的编码 ret.encoding=ret.apparent_encoding 解析json # 返回数据，有可能是json格式，有可能是html格式 ret=requests.get('http://0.0.0.0:8001/') print(type(ret.text)) print(ret.text) # a=ret.json() print(a['name']) print(type(a)) 使用代理 proxies={} # 正向代理 # django如何拿到客户端ip地址 META.get(\"REMOTE_ADDR\") # 如何去获取代理，如何使用（用自己项目验收） # 使用代理有什么用 ret=requests.get('http://0.0.0.0:8001/',proxies={'http':'地址'}) print(type(ret.text)) print(ret.text) 异常处理 # 用try except捕获一下 就用它就型了：Exception 上传文件 (爬虫用的比较少，后台写服务，) file={'myfile':open(\"1.txt\",'rb')} ret=requests.post('http://0.0.0.0:8001/',files=file) print(ret.content) ","date":"2017-07-03","objectID":"/posts/spider-1%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/:0:2","tags":["爬虫","requests模块"],"title":"Request模块","uri":"/posts/spider-1%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/"},{"categories":null,"content":"爬梨视频 ############ # 2 爬取视频 ############# #categoryId=9 分类id #start=0 从哪个位置开始，每次加载12个 # https://www.pearvideo.com/category_loading.jsp?reqType=5\u0026categoryId=9\u0026start=0 import requests import re ret=requests.get('https://www.pearvideo.com/category_loading.jsp?reqType=5\u0026categoryId=9\u0026start=0') # print(ret.text) # 正则取解析 reg='\u003ca href=\"(.*?)\" class=\"vervideo-lilink actplay\"\u003e' video_urls=re.findall(reg,ret.text) print(video_urls) for url in video_urls: ret_detail=requests.get('https://www.pearvideo.com/'+url) reg='srcUrl=\"(.*?)\",vdoUrl=srcUrl' mp4_url=re.findall(reg,ret_detail.text)[0] #type:str # 下载视频 video_content=requests.get(mp4_url) video_name=mp4_url.rsplit('/',1)[-1] with open(video_name,'wb') as f: for line in video_content.iter_content(): f.write(line) ","date":"2017-07-03","objectID":"/posts/spider-1%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/:0:3","tags":["爬虫","requests模块"],"title":"Request模块","uri":"/posts/spider-1%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/"},{"categories":null,"content":"模拟登陆 ############ # 3 模拟登陆某网站 ############# import requests ret = requests.post('http://www.aa7a.cn/user.php', data={ 'username': '616564099@qq.com', 'password': 'lqz123', 'captcha': 'f5jn', 'remember': '1', 'ref': 'http://www.aa7a.cn/', 'act': 'act_login', }) cookie=ret.cookies.get_dict() print(cookie) # 如果不出意外，咱么就登陆上了,再向首页发请求，首页返回的数据中就有616564099@qq.com ret1=requests.get('http://www.aa7a.cn/',cookies=cookie) # ret1=requests.get('http://www.aa7a.cn/') print('616564099@qq.com' in ret1.text) # 秒杀小米手机，一堆小号 # 定时任务：一到时间，就可以发送post请求，秒杀手机 # 以后碰到特别难登陆的网站，代码登陆不进去怎么办？ # 之所以要登陆，就是为了拿到cookie，下次发请求（如果程序拿不到cookie，自动登陆不进去） # 就手动登陆进去，然后用程序发请求 ","date":"2017-07-03","objectID":"/posts/spider-1%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/:0:4","tags":["爬虫","requests模块"],"title":"Request模块","uri":"/posts/spider-1%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/"},{"categories":["异步"],"content":"celery 的简单使用 ","date":"2017-06-03","objectID":"/posts/celery/:0:0","tags":["celery","FixIt"],"title":"Celery","uri":"/posts/celery/"},{"categories":["异步"],"content":"官方 Celery 官网：http://www.celeryproject.org/ Celery 官方文档英文版：http://docs.celeryproject.org/en/latest/index.html Celery 官方文档中文版：http://docs.jinkan.org/docs/celery/ ","date":"2017-06-03","objectID":"/posts/celery/:1:0","tags":["celery","FixIt"],"title":"Celery","uri":"/posts/celery/"},{"categories":["异步"],"content":"Celery异步任务框架 \"\"\" 1）可以不依赖任何服务器，通过自身命令，启动服务(内部支持socket) 2）celery服务为为其他项目服务提供异步解决任务需求的 注：会有两个服务同时运行，一个是项目服务，一个是celery服务，项目服务将需要异步处理的任务交给celery服务，celery就会在需要时异步完成项目的需求 人是一个独立运行的服务 | 医院也是一个独立运行的服务 正常情况下，人可以完成所有健康情况的动作，不需要医院的参与；但当人生病时，就会被医院接收，解决人生病问题 人生病的处理方案交给医院来解决，所有人不生病时，医院独立运行，人生病时，医院就来解决人生病的需求 \"\"\" ","date":"2017-06-03","objectID":"/posts/celery/:2:0","tags":["celery","FixIt"],"title":"Celery","uri":"/posts/celery/"},{"categories":["异步"],"content":"Celery架构 Celery的架构由三部分组成，消息中间件（message broker）、任务执行单元（worker）和 任务执行结果存储（task result store）组成。 ","date":"2017-06-03","objectID":"/posts/celery/:3:0","tags":["celery","FixIt"],"title":"Celery","uri":"/posts/celery/"},{"categories":["异步"],"content":"消息中间件 Celery本身不提供消息服务，但是可以方便的和第三方提供的消息中间件集成。包括，RabbitMQ, Redis等等 ","date":"2017-06-03","objectID":"/posts/celery/:3:1","tags":["celery","FixIt"],"title":"Celery","uri":"/posts/celery/"},{"categories":["异步"],"content":"任务执行单元 Worker是Celery提供的任务执行的单元，worker并发的运行在分布式的系统节点中。 ","date":"2017-06-03","objectID":"/posts/celery/:3:2","tags":["celery","FixIt"],"title":"Celery","uri":"/posts/celery/"},{"categories":["异步"],"content":"任务结果存储 Task result store用来存储Worker执行的任务的结果，Celery支持以不同方式存储任务的结果，包括AMQP, redis等 ","date":"2017-06-03","objectID":"/posts/celery/:3:3","tags":["celery","FixIt"],"title":"Celery","uri":"/posts/celery/"},{"categories":["异步"],"content":"使用场景 异步执行：解决耗时任务 延迟执行：解决延迟任务 定时执行：解决周期(周期)任务 ","date":"2017-06-03","objectID":"/posts/celery/:3:4","tags":["celery","FixIt"],"title":"Celery","uri":"/posts/celery/"},{"categories":["异步"],"content":"Celery的安装配置 pip install celery 消息中间件：RabbitMQ/Redis app=Celery(‘任务名’, broker=‘xxx’, backend=‘xxx’) ","date":"2017-06-03","objectID":"/posts/celery/:4:0","tags":["celery","FixIt"],"title":"Celery","uri":"/posts/celery/"},{"categories":["异步"],"content":"两种celery任务结构：提倡用包管理，结构更清晰 # 如果 Celery对象:Celery(...) 是放在一个模块下的 # 1）终端切换到该模块所在文件夹位置：scripts # 2）执行启动worker的命令：celery worker -A 模块名 -l info -P eventlet # 注：windows系统需要eventlet支持，Linux与MacOS直接执行：celery worker -A 模块名 -l info # 注：模块名随意 # 如果 Celery对象:Celery(...) 是放在一个包下的 # 1）必须在这个包下建一个celery.py的文件，将Celery(...)产生对象的语句放在该文件中 # 2）执行启动worker的命令：celery worker -A 包名 -l info -P eventlet # 注：windows系统需要eventlet支持，Linux与MacOS直接执行：celery worker -A 模块名 -l info # 注：包名随意 ","date":"2017-06-03","objectID":"/posts/celery/:4:1","tags":["celery","FixIt"],"title":"Celery","uri":"/posts/celery/"},{"categories":["异步"],"content":"Celery执行异步任务 ","date":"2017-06-03","objectID":"/posts/celery/:5:0","tags":["celery","FixIt"],"title":"Celery","uri":"/posts/celery/"},{"categories":["异步"],"content":"包架构封装 project ├── celery_task # celery包 │ ├── __init__.py # 包文件 │ ├── celery.py # celery连接和配置相关文件，且名字必须交celery.py │ └── tasks.py # 所有任务函数 ├── add_task.py # 添加任务 └── get_result.py # 获取结果 ","date":"2017-06-03","objectID":"/posts/celery/:5:1","tags":["celery","FixIt"],"title":"Celery","uri":"/posts/celery/"},{"categories":["异步"],"content":"基本使用 celery.py # 1）创建app + 任务 # 2）启动celery(app)服务： # 非windows # 命令：celery worker -A celery_task -l info # windows： # pip3 install eventlet # celery worker -A celery_task -l info -P eventlet # 3）添加任务：手动添加，要自定义添加任务的脚本，右键执行脚本 # 4）获取结果：手动获取，要自定义获取任务的脚本，右键执行脚本 from celery import Celery broker = 'redis://127.0.0.1:6379/1' backend = 'redis://127.0.0.1:6379/2' app = Celery(broker=broker, backend=backend, include=['celery_task.tasks']) tasks.py from .celery import app import time @app.task def add(n, m): print(n) print(m) time.sleep(10) print('n+m的结果：%s' % (n + m)) return n + m @app.task def low(n, m): print(n) print(m) print('n-m的结果：%s' % (n - m)) return n - m add_task.py from celery_task import tasks # 添加立即执行任务 t1 = tasks.add.delay(10, 20) t2 = tasks.low.delay(100, 50) print(t1.id) # 添加延迟任务 from datetime import datetime, timedelta eta=datetime.utcnow() + timedelta(seconds=10) tasks.low.apply_async(args=(200, 50), eta=eta) get_result.py from celery_task.celery import app from celery.result import AsyncResult id = '21325a40-9d32-44b5-a701-9a31cc3c74b5' if __name__ == '__main__': async = AsyncResult(id=id, app=app) if async.successful(): result = async.get() print(result) elif async.failed(): print('任务失败') elif async.status == 'PENDING': print('任务等待中被执行') elif async.status == 'RETRY': print('任务异常后正在重试') elif async.status == 'STARTED': print('任务已经开始被执行') ","date":"2017-06-03","objectID":"/posts/celery/:5:2","tags":["celery","FixIt"],"title":"Celery","uri":"/posts/celery/"},{"categories":["异步"],"content":"高级使用 celery.py # 1）创建app + 任务 # 2）启动celery(app)服务： # 非windows # 命令：celery worker -A celery_task -l info # windows： # pip3 install eventlet # celery worker -A celery_task -l info -P eventlet # 3）添加任务：自动添加任务，所以要启动一个添加任务的服务 # 命令：celery beat -A celery_task -l info # 4）获取结果 from celery import Celery broker = 'redis://127.0.0.1:6379/1' backend = 'redis://127.0.0.1:6379/2' app = Celery(broker=broker, backend=backend, include=['celery_task.tasks']) # 时区 app.conf.timezone = 'Asia/Shanghai' # 是否使用UTC app.conf.enable_utc = False # 任务的定时配置 from datetime import timedelta from celery.schedules import crontab app.conf.beat_schedule = { 'low-task': { 'task': 'celery_task.tasks.low', 'schedule': timedelta(seconds=3), # 'schedule': crontab(hour=8, day_of_week=1), # 每周一早八点 'args': (300, 150), } } tasks.py from .celery import app import time @app.task def add(n, m): print(n) print(m) time.sleep(10) print('n+m的结果：%s' % (n + m)) return n + m @app.task def low(n, m): print(n) print(m) print('n-m的结果：%s' % (n - m)) return n - m get_result.py from celery_task.celery import app from celery.result import AsyncResult id = '21325a40-9d32-44b5-a701-9a31cc3c74b5' if __name__ == '__main__': async = AsyncResult(id=id, app=app) if async.successful(): result = async.get() print(result) elif async.failed(): print('任务失败') elif async.status == 'PENDING': print('任务等待中被执行') elif async.status == 'RETRY': print('任务异常后正在重试') elif async.status == 'STARTED': print('任务已经开始被执行') ","date":"2017-06-03","objectID":"/posts/celery/:5:3","tags":["celery","FixIt"],"title":"Celery","uri":"/posts/celery/"},{"categories":["异步"],"content":"django中使用 celery.py \"\"\" celery框架django项目工作流程 1）加载django配置环境 2）创建Celery框架对象app，配置broker和backend，得到的app就是worker 3）给worker对应的app添加可处理的任务函数，用include配置给worker的app 4）完成提供的任务的定时配置app.conf.beat_schedule 5）启动celery服务，运行worker，执行任务 6）启动beat服务，运行beat，添加任务 重点：由于采用了django的反射机制，使用celery.py所在的celery_task包必须放置项目的根目录下 \"\"\" # 一、加载django配置环境 import os os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"luffyapi.settings.dev\") # 二、加载celery配置环境 from celery import Celery # broker broker = 'redis://127.0.0.1:6379/0' # backend backend = 'redis://127.0.0.1:6379/1' # worker app = Celery(broker=broker, backend=backend, include=['celery_task.tasks']) # 时区 app.conf.timezone = 'Asia/Shanghai' # 是否使用UTC app.conf.enable_utc = False # 任务的定时配置 from datetime import timedelta from celery.schedules import crontab app.conf.beat_schedule = { 'update-banner-list': { 'task': 'celery_task.tasks.update_banner_list', 'schedule': timedelta(seconds=10), 'args': (), } } tasks.py from .celery import app from django.core.cache import cache from home import models, serializers from django.conf import settings @app.task def update_banner_list(): queryset = models.Banner.objects.filter(is_delete=False, is_show=True).order_by('-orders')[:settings.BANNER_COUNT] banner_list = serializers.BannerSerializer(queryset, many=True).data # 拿不到request对象，所以头像的连接base_url要自己组装 for banner in banner_list: banner['image'] = 'http://127.0.0.1:8000%s' % banner['image'] cache.set('banner_list', banner_list, 86400) return True ","date":"2017-06-03","objectID":"/posts/celery/:5:4","tags":["celery","FixIt"],"title":"Celery","uri":"/posts/celery/"}]