[{"categories":["ElasticSearch"],"content":"Elasticsearch 常用的es 查询 ","date":"2020-09-05","objectID":"/posts/elasticsearch-08-term%E4%B8%8Ematch/:0:0","tags":["配置"],"title":"Elasticsearch term 与 match","uri":"/posts/elasticsearch-08-term%E4%B8%8Ematch/"},{"categories":["ElasticSearch"],"content":"准备数据 PUT zhifou/doc/1 { \"name\":\"顾老二\", \"age\":30, \"from\": \"gu\", \"desc\": \"皮肤黑、武器长、性格直\", \"tags\": [\"黑\", \"长\", \"直\"] } PUT zhifou/doc/2 { \"name\":\"大娘子\", \"age\":18, \"from\":\"sheng\", \"desc\":\"肤白貌美，娇憨可爱\", \"tags\":[\"白\", \"富\",\"美\"] } PUT zhifou/doc/3 { \"name\":\"龙套偏房\", \"age\":22, \"from\":\"gu\", \"desc\":\"mmp，没怎么看，不知道怎么形容\", \"tags\":[\"造数据\", \"真\",\"难\"] } PUT zhifou/doc/4 { \"name\":\"石头\", \"age\":29, \"from\":\"gu\", \"desc\":\"粗中有细，狐假虎威\", \"tags\":[\"粗\", \"大\",\"猛\"] } PUT zhifou/doc/5 { \"name\":\"魏行首\", \"age\":25, \"from\":\"广云台\", \"desc\":\"仿佛兮若轻云之蔽月,飘飘兮若流风之回雪,mmp，最后竟然没有嫁给顾老二！\", \"tags\":[\"闭月\",\"羞花\"] } ","date":"2020-09-05","objectID":"/posts/elasticsearch-08-term%E4%B8%8Ematch/:0:1","tags":["配置"],"title":"Elasticsearch term 与 match","uri":"/posts/elasticsearch-08-term%E4%B8%8Ematch/"},{"categories":["ElasticSearch"],"content":"Elasticsearch 简单操作 ","date":"2020-09-05","objectID":"/posts/elasticsearch-08-es%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C/:0:0","tags":["配置"],"title":"Elasticsearch 简单操作","uri":"/posts/elasticsearch-08-es%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C/"},{"categories":["ElasticSearch"],"content":"前言 现在，让我们启动一个节点和kibana。 接下来的一切操作都在kibana中Dev Tools下的Console里完成。 ","date":"2020-09-05","objectID":"/posts/elasticsearch-08-es%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C/:1:0","tags":["配置"],"title":"Elasticsearch 简单操作","uri":"/posts/elasticsearch-08-es%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C/"},{"categories":["ElasticSearch"],"content":"创建一篇文档 现在，我们试图将小黑的小姨妈的个人信息录入elasticsearch。我们只要输入： PUT t1/doc/1 { \"name\": \"小黑的小姨妈\", \"age\": 18 } PUT表示创建命令。虽然命令可以小写，但是我们推荐大写。在以REST ful风格返回的结果中： { \"_index\" : \"t1\", \"_type\" : \"type1\", \"_id\" : \"1\", \"_version\" : 1, \"result\" : \"created\", \"_shards\" : { \"total\" : 2, \"successful\" : 1, \"failed\" : 0 }, \"_seq_no\" : 0, \"_primary_term\" : 1 } 结果中的result则是操作类型，现在是created，表示第一次创建。如果我们再次点击执行该命令，那么result则会是updated。我们细心则会发现_version开始是1，现在你每点击一次就会增加一次。表示第几次更改。 ","date":"2020-09-05","objectID":"/posts/elasticsearch-08-es%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C/:1:1","tags":["配置"],"title":"Elasticsearch 简单操作","uri":"/posts/elasticsearch-08-es%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C/"},{"categories":["ElasticSearch"],"content":"查询所有索引 现在，我们再来学习一条命令： GET _cat/indices?v 返回的结果如下图： 上图中，展示当前集群中索引情况，包括，索引的健康状况、UUID、主副分片个数、大小等信息。你发现我们创建的t1索引了吗？ ","date":"2020-09-05","objectID":"/posts/elasticsearch-08-es%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C/:1:2","tags":["配置"],"title":"Elasticsearch 简单操作","uri":"/posts/elasticsearch-08-es%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C/"},{"categories":["ElasticSearch"],"content":"查询指定的索引信息 我们来单独看看t1索引： GET t1 返回的结果如下： { \"t1\" : { \"aliases\" : { }, \"mappings\" : { \"doc\" : { \"properties\" : { \"age\" : { \"type\" : \"long\" }, \"name\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } } } } }, \"settings\" : { \"index\" : { \"creation_date\" : \"1553163739688\", \"number_of_shards\" : \"5\", \"number_of_replicas\" : \"1\", \"uuid\" : \"_7jNW5XATheeK84zKkPwlw\", \"version\" : { \"created\" : \"6050499\" }, \"provided_name\" : \"t1\" } } } } 返回了t1索引的创建信息。 ","date":"2020-09-05","objectID":"/posts/elasticsearch-08-es%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C/:1:3","tags":["配置"],"title":"Elasticsearch 简单操作","uri":"/posts/elasticsearch-08-es%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C/"},{"categories":["ElasticSearch"],"content":"查询文档信息 那我们来查看我们刚才创建的那篇文档： GET t1/doc/1 返回的结果如下： Copy{ \"_index\" : \"t1\", \"_type\" : \"doc\", \"_id\" : \"1\", \"_version\" : 2, \"found\" : true, \"_source\" : { \"name\" : \"小黑的小姨妈\", \"age\" : 18 } } 返回了我们刚才创建的文档信息。 我们再来为小黑添加两个姨妈： CopyPUT t1/doc/2 { \"name\": \"小黑的二姨妈\", \"age\": 16 } PUT t1/doc/3 { \"name\": \"小黑的三姨妈\", \"age\": 19 } 刚才，我们学会了查询小黑的一个姨妈，那么该如何查询所有姨妈呢？ GET t1/doc/_search 返回结果如下： Copy{ \"took\" : 7, \"timed_out\" : false, \"_shards\" : { \"total\" : 5, \"successful\" : 5, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : 3, \"max_score\" : 1.0, \"hits\" : [ { \"_index\" : \"t1\", \"_type\" : \"doc\", \"_id\" : \"2\", \"_score\" : 1.0, \"_source\" : { \"name\" : \"小黑的二姨妈\", \"age\" : 16 } }, { \"_index\" : \"t1\", \"_type\" : \"doc\", \"_id\" : \"1\", \"_score\" : 1.0, \"_source\" : { \"name\" : \"小黑的小姨妈\", \"age\" : 18 } }, { \"_index\" : \"t1\", \"_type\" : \"doc\", \"_id\" : \"3\", \"_score\" : 1.0, \"_source\" : { \"name\" : \"小黑的三姨妈\", \"age\" : 19 } } ] } } 现在小黑跟他的姨妈们闹了别扭，就想删除这个姨妈，该怎么办呢？ ","date":"2020-09-05","objectID":"/posts/elasticsearch-08-es%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C/:1:4","tags":["配置"],"title":"Elasticsearch 简单操作","uri":"/posts/elasticsearch-08-es%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C/"},{"categories":["ElasticSearch"],"content":"删除指定索引 我们其实直接删除这个t1索引就可以了： DELETE /t1 DELETE 是删除命令，返回结果如下： { \"acknowledged\" : true } 返回结果提示删除确认成功。 如果此时再查询索引情况，则会发现t1已经不存在了，所有的文档也就不存在了。 欢迎斧正，that’s all ","date":"2020-09-05","objectID":"/posts/elasticsearch-08-es%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C/:1:5","tags":["配置"],"title":"Elasticsearch 简单操作","uri":"/posts/elasticsearch-08-es%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C/"},{"categories":["ElasticSearch"],"content":"Elasticsearch 与Kibana通讯及 es-head 开源管理项目 ","date":"2020-09-04","objectID":"/posts/elasticsearch-07-kibana-%E4%B8%8E-es-%E8%BF%9E%E6%8E%A5/:0:0","tags":["配置"],"title":"Elasticsearch 与Kibana通讯及 es-head 开源管理项目","uri":"/posts/elasticsearch-07-kibana-%E4%B8%8E-es-%E8%BF%9E%E6%8E%A5/"},{"categories":["ElasticSearch"],"content":"Docker 配置桥接 #　创建网桥 docker network create -d bridge my-bridge # es 连接　网桥 docker network connect my-bridge es-a # kibana 连接网桥 docker network connect my-bridge kibana ","date":"2020-09-04","objectID":"/posts/elasticsearch-07-kibana-%E4%B8%8E-es-%E8%BF%9E%E6%8E%A5/:0:1","tags":["配置"],"title":"Elasticsearch 与Kibana通讯及 es-head 开源管理项目","uri":"/posts/elasticsearch-07-kibana-%E4%B8%8E-es-%E8%BF%9E%E6%8E%A5/"},{"categories":["ElasticSearch"],"content":"修改kibana　配置文件 # 查看　es-a　容器　所在　公用网桥中的　ip sudo docker inspect es-a \"my-bridge\": { \"IPAMConfig\": {}, \"Links\": null, \"Aliases\": [ \"9f05d30f9f9e\" ], \"Gateway\": \"172.19.0.1\", \"IPAddress\": \"172.19.0.2\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"MacAddress\": \"02:42:ac:13:00:02\", \"DriverOpts\": {} } # 可以看到　在网桥中的　ip　为　\"IPAddress\": \"172.19.0.2\", # kibana 配置文件　设置　es 容器所在的ip elasticsearch.hosts: [\"http://172.19.0.2:9200\"] # 设置完成后　需要重启容器　才能生效（真香） ","date":"2020-09-04","objectID":"/posts/elasticsearch-07-kibana-%E4%B8%8E-es-%E8%BF%9E%E6%8E%A5/:0:2","tags":["配置"],"title":"Elasticsearch 与Kibana通讯及 es-head 开源管理项目","uri":"/posts/elasticsearch-07-kibana-%E4%B8%8E-es-%E8%BF%9E%E6%8E%A5/"},{"categories":["ElasticSearch"],"content":"使用elasticsearch-head　开元项目对es进行管理 git clone https://github.com/mobz/elasticsearch-head.git cd elasticsearch-head npm install npm run start open http://localhost:9100/ ","date":"2020-09-04","objectID":"/posts/elasticsearch-07-kibana-%E4%B8%8E-es-%E8%BF%9E%E6%8E%A5/:0:3","tags":["配置"],"title":"Elasticsearch 与Kibana通讯及 es-head 开源管理项目","uri":"/posts/elasticsearch-07-kibana-%E4%B8%8E-es-%E8%BF%9E%E6%8E%A5/"},{"categories":["ElasticSearch"],"content":"ElasticSearch　解决跨域问题(head　与　es 连接问题) # 在es 配置文件中　加入以下配置　解决跨域问题 http.cors.enabled: true http.cors.allow-origin: \"*\" ","date":"2020-09-04","objectID":"/posts/elasticsearch-07-kibana-%E4%B8%8E-es-%E8%BF%9E%E6%8E%A5/:0:4","tags":["配置"],"title":"Elasticsearch 与Kibana通讯及 es-head 开源管理项目","uri":"/posts/elasticsearch-07-kibana-%E4%B8%8E-es-%E8%BF%9E%E6%8E%A5/"},{"categories":["ElasticSearch"],"content":"Elasticsearch 配置文件详解 ##################### Elasticsearch Configuration Example ##################### # # 只是挑些重要的配置选项进行注释,其实自带的已经有非常细致的英文注释了! # https://www.elastic.co/guide/en/elasticsearch/reference/current/modules.html # ################################### Cluster ################################### # 代表一个集群,集群中有多个节点,其中有一个为主节点,这个主节点是可以通过选举产生的,主从节点是对于集群内部来说的. # es的一个概念就是去中心化,字面上理解就是无中心节点,这是对于集群外部来说的,因为从外部来看es集群,在逻辑上是个整体,你与任何一个节点的通信和与整个es集群通信是等价的。 # cluster.name可以确定你的集群名称,当你的elasticsearch集群在同一个网段中elasticsearch会自动的找到具有相同cluster.name的elasticsearch服务. # 所以当同一个网段具有多个elasticsearch集群时cluster.name就成为同一个集群的标识. # cluster.name: elasticsearch #################################### Node ##################################### # https://www.elastic.co/guide/en/elasticsearch/reference/5.1/modules-node.html#master-node # 节点名称同理,可自动生成也可手动配置. # node.name: node-1 # 允许一个节点是否可以成为一个master节点,es是默认集群中的第一台机器为master,如果这台机器停止就会重新选举master. # node.master: true # 允许该节点存储数据(默认开启) # node.data: true # 配置文件中给出了三种配置高性能集群拓扑结构的模式,如下： # 1. 如果你想让节点从不选举为主节点,只用来存储数据,可作为负载器 # node.master: false # node.data: true # node.ingest: false # 2. 如果想让节点成为主节点,且不存储任何数据,并保有空闲资源,可作为协调器 # node.master: true # node.data: false # node.ingest: false # 3. 如果想让节点既不称为主节点,又不成为数据节点,那么可将他作为搜索器,从节点中获取数据,生成搜索结果等 # node.master: false # node.data: false # node.ingest: true (可不指定默认开启) # 4. 仅作为协调器 # node.master: false # node.data: false # node.ingest: false # 监控集群状态有一下插件和API可以使用: # Use the Cluster Health API [http://localhost:9200/_cluster/health], the # Node Info API [http://localhost:9200/_nodes] or GUI tools # such as \u003chttp://www.elasticsearch.org/overview/marvel/\u003e, # \u003chttp://github.com/karmi/elasticsearch-paramedic\u003e, # \u003chttp://github.com/lukas-vlcek/bigdesk\u003e and # \u003chttp://mobz.github.com/elasticsearch-head\u003e to inspect the cluster state. # A node can have generic attributes associated with it, which can later be used # for customized shard allocation filtering, or allocation awareness. An attribute # is a simple key value pair, similar to node.key: value, here is an example: # 每个节点都可以定义一些与之关联的通用属性，用于后期集群进行碎片分配时的过滤 # node.rack: rack314 # 默认情况下，多个节点可以在同一个安装路径启动，如果你想让你的es只启动一个节点，可以进行如下设置 # node.max_local_storage_nodes: 1 #################################### Index #################################### # 设置索引的分片数,默认为5 #index.number_of_shards: 5 # 设置索引的副本数,默认为1: #index.number_of_replicas: 1 # 配置文件中提到的最佳实践是,如果服务器够多,可以将分片提高,尽量将数据平均分布到大集群中去 # 同时,如果增加副本数量可以有效的提高搜索性能 # 需要注意的是,\"number_of_shards\" 是索引创建后一次生成的,后续不可更改设置 # \"number_of_replicas\" 是可以通过API去实时修改设置的 #################################### Paths #################################### # 配置文件存储位置 # path.conf: /path/to/conf # 数据存储位置(单个目录设置) # path.data: /path/to/data # 多个数据存储位置,有利于性能提升 # path.data: /path/to/data1,/path/to/data2 # 临时文件的路径 # path.work: /path/to/work # 日志文件的路径 # path.logs: /path/to/logs # 插件安装路径 # path.plugins: /path/to/plugins #################################### Plugin ################################### # 设置插件作为启动条件,如果一下插件没有安装,则该节点服务不会启动 # plugin.mandatory: mapper-attachments,lang-groovy ################################### Memory #################################### # 当JVM开始写入交换空间时（swapping）ElasticSearch性能会低下,你应该保证它不会写入交换空间 # 设置这个属性为true来锁定内存,同时也要允许elasticsearch的进程可以锁住内存,linux下可以通过 `ulimit -l unlimited` 命令 # bootstrap.mlockall: true # 确保 ES_MIN_MEM 和 ES_MAX_MEM 环境变量设置为相同的值,以及机器有足够的内存分配给Elasticsearch # 注意:内存也不是越大越好,一般64位机器,最大分配内存别才超过32G ############################## Network And HTTP ############################### # 设置绑定的ip地址,可以是ipv4或ipv6的,默认为0.0.0.0 # network.bind_host: 192.168.0.1 # 设置其它节点和该节点交互的ip地址,如果不设置它会自动设置,值必须是个真实的ip地址 # network.publish_host: 192.168.0.1 # 同时设置bind_host和publish_host上面两个参数 # network.host: 192.168.0.1 # 设置节点间交互的tcp端口,默认是9300 # transport.tcp.port: 9300 # 设置是否压缩tcp传输时的数据，默认为false,不压缩 # transport.tcp.compress: true # 设置对外服务的http端口,默认为9200 # http.port: 9200 # 设置请求内容的最大容量,默认100mb # http.max_content_length: 100mb # 使用http协议对外提供服务,默认为true,开启 # http.enabled: false ###################### 使用head等插件监控集群信息，需要打开以下配置项 ########### # http.c","date":"2020-09-03","objectID":"/posts/elasticsearch-06-es-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3/:0:0","tags":["配置"],"title":"Elasticsearch 配置文件详解","uri":"/posts/elasticsearch-06-es-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3/"},{"categories":["ElasticSearch"],"content":"最新配置 cluster.name: elasticsearch # 配置的集群名称，默认是elasticsearch，es服务会通过广播方式自动连接在同一网段下的es服务，通过多播方式进行通信，同一网段下可以有多个集群，通过集群名称这个属性来区分不同的集群 node.name: \"Franz Kafka\" # 当前配置所在机器的节点名 node.master: true 指定该节点是否有资格被选举成为node（注意这里只是设置成有资格),默认true node.data: true # 指定该节点是否存储索引数据，默认为true。 index.number_of_shards: 5 # 设置默认索引分片个数，默认为5片。7.X版本以上配置无效 index.number_of_replicas: 1 # 设置默认索引副本个数，默认为1个副本。7.x版本以上配置无效 path.conf: /path/to/conf # 设置配置文件的存储路径，默认是es根目录下的config文件夹。 path.data: /path/to/data # 设置索引数据的存储路径 path.work: /path/to/work # 设置临时文件的存储路径 path.logs: /path/to/logs # 设置日志文件的存储路径 path.plugins: /path/to/plugins # 设置插件的存放路径 network.bind_host: 192.168.0.1 # 设置绑定的ip地址，可以是ipv4或ipv6的，默认为0.0.0.0，绑定这台机器的任何一个ip。 network.publish_host: 192.168.0.1 # 设置其它节点和该节点交互的ip地址，如果不设置它会自动判断，值必须是个真实的ip地址。 network.host: 192.168.0.1 # 这个参数是用来同时设置bind_host和publish_host上面两个参数。 transport.tcp.port: 9300 # 设置节点之间交互的tcp端口，默认是9300。 transport.tcp.compress: true # 设置是否压缩tcp传输时的数据，默认为false，不压缩。 http.port: 9200 # 设置对外服务的http端口，默认为9200。 http.max_content_length: 100mb # 设置内容的最大容量，默认100mb http.enabled: false # 是否使用http协议对外提供服务，默认为true，开启。 gateway.type: local # gateway的类型，默认为local即为本地文件系统，可以设置为本地文件系统，分布式文件系统，hadoop的HDFS，和amazon的s3服务器等。 cluster.routing.allocation.node_initial_primaries_recoveries: 4 # 初始化数据恢复时，并发恢复线程的个数，默认为4。 cluster.routing.allocation.node_concurrent_recoveries: 2 # 添加删除节点或负载均衡时并发恢复线程的个数，默认为4。 indices.recovery.max_size_per_sec: 0 # 设置数据恢复时限制的带宽，如入100mb，默认为0，即无限制。 indices.recovery.concurrent_streams: 5 # 设置这个参数来限制从其它分片恢复数据时最大同时打开并发流的个数，默认为5。 discovery.zen.minimum_master_nodes: 1 # 设置这个参数来保证集群中的节点可以知道其它N个有master资格的节点。默认为1，对于大的集群来说，可以设置大一点的值（2-4）。7.x版本无效 discovery.zen.ping.timeout: 3s # 设置集群中自动发现其它节点时ping连接超时时间，默认为3秒，对于比较差的网络环境可以高点的值来防止自动发现时出错。7.x版本无效 discovery.zen.ping.multicast.enabled: false # 设置是否打开多播发现节点，默认是true。7.x版本无效 discovery.zen.ping.unicast.hosts: [\"host1\", \"host2:port\", \"host3[portX-portY]\"] # 设置集群中master节点的初始列表，可以通过这些节点来自动发现新加入集群的节点。7.x版本无效 discovery.seed_hosts # 在启动此节点时传递要执行发现的主机的初始列表 cluster.initial_master_nodes # 使用初始的一组符合主节点条件的节点引导集群 ","date":"2020-09-03","objectID":"/posts/elasticsearch-06-es-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3/:0:1","tags":["配置"],"title":"Elasticsearch 配置文件详解","uri":"/posts/elasticsearch-06-es-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3/"},{"categories":["ElasticSearch"],"content":"docker 容器安装 Kibana Kibana ","date":"2020-09-03","objectID":"/posts/elasticsearch-05-kibana/:0:0","tags":["Docker","Kibana"],"title":"Kibana Docker 容器安装","uri":"/posts/elasticsearch-05-kibana/"},{"categories":["ElasticSearch"],"content":"挂载 在宿主机上创建kibana config配置目录 eg: 配置文件 mkdir /home/zhang/Document/Kibana/config ","date":"2020-09-03","objectID":"/posts/elasticsearch-05-kibana/:1:0","tags":["Docker","Kibana"],"title":"Kibana Docker 容器安装","uri":"/posts/elasticsearch-05-kibana/"},{"categories":["ElasticSearch"],"content":"Kibana配置 在/home/zhang/Document/Kibana/config目录下创建kibana.yml，具体内容如下： # Kibana is served by a back end server. This setting specifies the port to use. server.port: 5602 # Specifies the address to which the Kibana server will bind. IP addresses and host names are both valid values. # The default is 'localhost', which usually means remote machines will not be able to connect. # To allow connections from remote users, set this parameter to a non-loopback address. server.host: 0.0.0.0 # Enables you to specify a path to mount Kibana at if you are running behind a proxy. # Use the `server.rewriteBasePath` setting to tell Kibana if it should remove the basePath # from requests it receives, and to prevent a deprecation warning at startup. # This setting cannot end in a slash. #server.basePath: \"\" # Specifies whether Kibana should rewrite requests that are prefixed with # `server.basePath` or require that they are rewritten by your reverse proxy. # This setting was effectively always `false` before Kibana 6.3 and will # default to `true` starting in Kibana 7.0. #server.rewriteBasePath: false # The maximum payload size in bytes for incoming server requests. #server.maxPayloadBytes: 1048576 # The Kibana server's name. This is used for display purposes. #server.name: \"your-hostname\" # The URLs of the Elasticsearch instances to use for all your queries. elasticsearch.hosts: [\"http://10.133.0.188:9201\"] # When this setting's value is true Kibana uses the hostname specified in the server.host # setting. When the value of this setting is false, Kibana uses the hostname of the host # that connects to this Kibana instance. #elasticsearch.preserveHost: true # Kibana uses an index in Elasticsearch to store saved searches, visualizations and # dashboards. Kibana creates a new index if the index doesn't already exist. #kibana.index: \".kibana\" # The default application to load. #kibana.defaultAppId: \"home\" # If your Elasticsearch is protected with basic authentication, these settings provide # the username and password that the Kibana server uses to perform maintenance on the Kibana # index at startup. Your Kibana users still need to authenticate with Elasticsearch, which # is proxied through the Kibana server. #elasticsearch.username: \"kibana\" #elasticsearch.password: \"pass\" # Enables SSL and paths to the PEM-format SSL certificate and SSL key files, respectively. # These settings enable SSL for outgoing requests from the Kibana server to the browser. #server.ssl.enabled: false #server.ssl.certificate: /path/to/your/server.crt #server.ssl.key: /path/to/your/server.key # Optional settings that provide the paths to the PEM-format SSL certificate and key files. # These files are used to verify the identity of Kibana to Elasticsearch and are required when # xpack.security.http.ssl.client_authentication in Elasticsearch is set to required. #elasticsearch.ssl.certificate: /path/to/your/client.crt #elasticsearch.ssl.key: /path/to/your/client.key # Optional setting that enables you to specify a path to the PEM file for the certificate # authority for your Elasticsearch instance. #elasticsearch.ssl.certificateAuthorities: [ \"/path/to/your/CA.pem\" ] # To disregard the validity of SSL certificates, change this setting's value to 'none'. #elasticsearch.ssl.verificationMode: full # Time in milliseconds to wait for Elasticsearch to respond to pings. Defaults to the value of # the elasticsearch.requestTimeout setting. #elasticsearch.pingTimeout: 1500 # Time in milliseconds to wait for responses from the back end or Elasticsearch. This value # must be a positive integer. #elasticsearch.requestTimeout: 30000 # List of Kibana client-side headers to send to Elasticsearch. To send *no* client-side # headers, set this value to [](an empty list). #elasticsearch.requestHeadersWhitelist: [ authorization ] # Header names and values that are sent to Elasticsearch. Any custom headers cannot be overwritten # by client-side headers, regardless of the elasticsearch.requestHeaders","date":"2020-09-03","objectID":"/posts/elasticsearch-05-kibana/:2:0","tags":["Docker","Kibana"],"title":"Kibana Docker 容器安装","uri":"/posts/elasticsearch-05-kibana/"},{"categories":["ElasticSearch"],"content":"启动 创建docker-compose.yml文件，内容如下： version: '2.2' services: kibana: image: kibana:7.6.2 volumes: - ./kibana.yml:/usr/share/kibana/config/kibana.yml ports: - '5602:5602/tcp' 启动： docker-compose up -d ","date":"2020-09-03","objectID":"/posts/elasticsearch-05-kibana/:3:0","tags":["Docker","Kibana"],"title":"Kibana Docker 容器安装","uri":"/posts/elasticsearch-05-kibana/"},{"categories":["ElasticSearch"],"content":"什么是ElasticSearch ","date":"2020-09-02","objectID":"/posts/elasticsearch-04-docker-%E9%9B%86%E7%BE%A4/:0:0","tags":["Docker"],"title":"使用docker安装Elasticsearch 集群","uri":"/posts/elasticsearch-04-docker-%E9%9B%86%E7%BE%A4/"},{"categories":["ElasticSearch"],"content":"Elasticsearch 以7.2.0为例 ","date":"2020-09-02","objectID":"/posts/elasticsearch-04-docker-%E9%9B%86%E7%BE%A4/:1:0","tags":["Docker"],"title":"使用docker安装Elasticsearch 集群","uri":"/posts/elasticsearch-04-docker-%E9%9B%86%E7%BE%A4/"},{"categories":["ElasticSearch"],"content":"挂载 在宿主机上创建Elasticsearch的数据存储、config配置目录 eg: # 在 Document 下创建 ElasticSearch 文件夹， 然后再创建 conf data logs 文件夹 用于放置 es的数据， 配置 ， 日志文件 mkdir ELasticSearch cd ElasticSearch mkdir config mkdir logs mkdir data 新建用户 adduser es 数据存储需要赋予权限：sudo chown -R 1000:1000 ~/ELasticSearch/data ","date":"2020-09-02","objectID":"/posts/elasticsearch-04-docker-%E9%9B%86%E7%BE%A4/:1:1","tags":["Docker"],"title":"使用docker安装Elasticsearch 集群","uri":"/posts/elasticsearch-04-docker-%E9%9B%86%E7%BE%A4/"},{"categories":["ElasticSearch"],"content":"Elasticsearch配置 在 ~/ELasticSearch/data/config文件夹下新建elasticsearch.yml、 jvm.options 具体内容可以参考： elasticsearch.yml cluster.name: business-log node.name: es-b-188 http.port: 9200 transport.tcp.port: 9301 network.bind_host: 10.133.0.188 network.publish_host: 10.133.0.188 jvm.options ## JVM configuration ################################################################ ## IMPORTANT: JVM heap size ################################################################ ## ## You should always set the min and max JVM heap ## size to the same value. For example, to set ## the heap to 4 GB, set: ## ## -Xms4g ## -Xmx4g ## ## See https://www.elastic.co/guide/en/elasticsearch/reference/current/heap-size.html ## for more information ## ################################################################ # Xms represents the initial size of total heap space # Xmx represents the maximum size of total heap space -Xms26g -Xmx26g ################################################################ ## Expert settings ################################################################ ## ## All settings below this section are considered ## expert settings. Don't tamper with them unless ## you understand what you are doing ## ################################################################ ## GC configuration #-XX:+UseConcMarkSweepGC #-XX:CMSInitiatingOccupancyFraction=75 #-XX:+UseCMSInitiatingOccupancyOnly ## G1GC Configuration # NOTE: G1GC is only supported on JDK version 10 or later. # To use G1GC uncomment the lines below. #-XX:-UseConcMarkSweepGC #-XX:-UseCMSInitiatingOccupancyOnly -XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=75 ## DNS cache policy # cache ttl in seconds for positive DNS lookups noting that this overrides the # JDK security property networkaddress.cache.ttl; set to -1 to cache forever -Des.networkaddress.cache.ttl=60 # cache ttl in seconds for negative DNS lookups noting that this overrides the # JDK security property networkaddress.cache.negative ttl; set to -1 to cache # forever -Des.networkaddress.cache.negative.ttl=10 ## optimizations # pre-touch memory pages used by the JVM during initialization -XX:+AlwaysPreTouch ## basic # explicitly set the stack size -Xss1m # set to headless, just in case -Djava.awt.headless=true # ensure UTF-8 encoding by default (e.g. filenames) -Dfile.encoding=UTF-8 # use our provided JNA always versus the system one -Djna.nosys=true # turn off a JDK optimization that throws away stack traces for common # exceptions because stack traces are important for debugging -XX:-OmitStackTraceInFastThrow # flags to configure Netty -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 # log4j 2 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Djava.io.tmpdir=${ES_TMPDIR} ## heap dumps # generate a heap dump when an allocation from the Java heap fails # heap dumps are created in the working directory of the JVM -XX:+HeapDumpOnOutOfMemoryError # specify an alternative path for heap dumps; ensure the directory exists and # has sufficient space -XX:HeapDumpPath=data # specify an alternative path for JVM fatal error logs -XX:ErrorFile=logs/hs_err_pid%p.log ## JDK 8 GC logging 8:-XX:+PrintGCDetails 8:-XX:+PrintGCDateStamps 8:-XX:+PrintTenuringDistribution 8:-XX:+PrintGCApplicationStoppedTime 8:-Xloggc:logs/gc.log 8:-XX:+UseGCLogFileRotation 8:-XX:NumberOfGCLogFiles=32 8:-XX:GCLogFileSize=64m # JDK 9+ GC logging 9-:-Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m # due to internationalization enhancements in JDK 9 Elasticsearch need to set the provider to COMPAT otherwise # time/date parsing will break in an incompatible way for some date patterns and locals 9-:-Djava.locale.providers=COMPAT 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161","date":"2020-09-02","objectID":"/posts/elasticsearch-04-docker-%E9%9B%86%E7%BE%A4/:1:2","tags":["Docker"],"title":"使用docker安装Elasticsearch 集群","uri":"/posts/elasticsearch-04-docker-%E9%9B%86%E7%BE%A4/"},{"categories":["ElasticSearch"],"content":"启动 创建docker-compose.yml文件，内容如下： version: '2.2' services: elasticsearch: image: elasticsearch:7.2.0 restart: always container_name: es-a network_mode: host volumes: - /home/zhang/Documents/ElasticSearch/data-a:/usr/share/elasticsearch/data - /home/zhang/Documents/ElasticSearch/config/el-a.yml:/usr/share/elasticsearch/config/elasticsearch.yml - /home/zhang/Documents/ElasticSearch/config/jvm.options:/usr/share/elasticsearch/config/jvm.options environment: - bootstrap.memory_lock=true - ES_JAVA_OPTS= -Xms64m -Xmx128m ulimits: memlock: soft: -1 hard: -1 启动： docker-compose up -d # 在使用 docker-compose 创建容器的过程中 创建端口映射不成功 所以使用一下命令创建， 可解决此问题。 sudo docker run -id --name=es-a -p 9200:9200 -p 9300:9300 -v /home/zhang/Documents/ElasticSearch/data/data-a:/usr/share/elasticsearch/data -v /home/zhang/Documents/ElasticSearch/config/el-a.yml:/usr/share/elasticsearch/config/elasticsearch.yml -v /home/zhang/Documents/ElasticSearch/config/jvm.options:/usr/share/elasticsearch/config/jvm.options -e \"discovery.type=single-node\" -e \"ES_JAVA_OPTS= -Xms1024m -Xmx1024m\" elasticsearch:7.2.0 ","date":"2020-09-02","objectID":"/posts/elasticsearch-04-docker-%E9%9B%86%E7%BE%A4/:1:3","tags":["Docker"],"title":"使用docker安装Elasticsearch 集群","uri":"/posts/elasticsearch-04-docker-%E9%9B%86%E7%BE%A4/"},{"categories":["ElasticSearch"],"content":"备注 elasticsearch.yml、jvm.options、docker-compose.yml等每个节点都需要配置，其中elasticsearch.yml、docker-compose.yml具体节点信息需要替换。 ","date":"2020-09-02","objectID":"/posts/elasticsearch-04-docker-%E9%9B%86%E7%BE%A4/:1:4","tags":["Docker"],"title":"使用docker安装Elasticsearch 集群","uri":"/posts/elasticsearch-04-docker-%E9%9B%86%E7%BE%A4/"},{"categories":["ElasticSearch"],"content":"什么是 Apache Lucene ","date":"2020-08-31","objectID":"/posts/elasticsearch-02-apache-lucene/:0:0","tags":["Lucene"],"title":"Apache Lucene","uri":"/posts/elasticsearch-02-apache-lucene/"},{"categories":["ElasticSearch"],"content":"有必要了解的有Apache Apache软件基金会（也就是Apache Software Foundation，简称为ASF）是专门为运作一个开源软件项目的Apache 的团体提供支持的非盈利性组织，这个开源软件的项目就是 Apache 项目。 最初，Apache基金会的开发爱好者开发并维护一个叫Apache的HTTP服务器。 后来，Apache服务器越来越火，就启动了更多的项目，比如PHP、Java Apache以及更多的子项目。比如Jakarta。 ","date":"2020-08-31","objectID":"/posts/elasticsearch-02-apache-lucene/:0:1","tags":["Lucene"],"title":"Apache Lucene","uri":"/posts/elasticsearch-02-apache-lucene/"},{"categories":["ElasticSearch"],"content":"Jakarta Jakarta是为了发展Java容器而启动的Java Apache的项目。后来随着Java的火爆而成为了囊括了众多基于Java语言开源软件子项目的项目。比如从这里孵化出了Tomcat、[ant](https://baike.baidu.com/item/apache ant/1065741?fr=aladdin)、Struts、Lucene。Jakarta ","date":"2020-08-31","objectID":"/posts/elasticsearch-02-apache-lucene/:0:2","tags":["Lucene"],"title":"Apache Lucene","uri":"/posts/elasticsearch-02-apache-lucene/"},{"categories":["ElasticSearch"],"content":"Lucene Lucene是Apache软件基金会4 jakarta项目的子项目。它是一个开源的全文检索引擎工具包。但它并不是一个完整的全文检索引擎，而是一个全文检索引擎的架构，提供了完整的查询引擎和索引引擎，部分文本分析引擎（英文与德文两种西方语言）。Lucene的目的是为软件开发人员提供一个简单易用的工具包，以方便的在目标系统中实现全文检索的功能，或者是以此为基础建立起完整的全文检索引擎。Lucene是一套用于全文检索和搜寻的开源程式库，由Apache软件基金会支持和提供。Lucene提供了一个简单却强大的应用程式接口，能够做全文索引和搜寻。在Java开发环境里Lucene是一个成熟的免费开源工具。 最后，引用来自《Elasticsearch权威指南》书中关于Lucene的描述作为总结： Lucene可以被认为是迄今为止最先进、性能最好的、功能最全的搜索引擎库。 但是，Lucene只是一个库。想要使用它，你必须使用Java来作为开发语言并将其直接集成到你的应用中，更糟糕的是，Lucene非常复杂，你需要深入了解检索的相关知识来理解它是如何工作的。 ","date":"2020-08-31","objectID":"/posts/elasticsearch-02-apache-lucene/:0:3","tags":["Lucene"],"title":"Apache Lucene","uri":"/posts/elasticsearch-02-apache-lucene/"},{"categories":["ElasticSearch"],"content":"常见的开源搜索引擎 基于Lucene的搜索引擎，Java开发，包括： Lucene Solr elasticsearch katta compass 基于C++开发的： Sphinx 你可以想想Lucene的强大。接下来简要的介绍各搜索引擎的特点。 Lucene Lucene的开发语言是Java，也是Java家族中最为出名的一个开源搜索引擎，在Java世界中已经是标准的全文检索程序，它提供了完整的查询引擎和索引引擎，没有中文分词引擎，需要自己去实现，因此用Lucene去做一个搜素引擎需要自己去架构，另外它不支持实时搜索。 优点： 成熟的解决方案，有很多的成功案例。apache 顶级项目，正在持续快速的进步。庞大而活跃的开发社区，大量的开发人员。它只是一个类库，有足够的定制和优化空间：经过简单定制，就可以满足绝大部分常见的需求；经过优化，可以支持 10亿+ 量级的搜索 缺点： 需要额外的开发工作。所有的扩展，分布式，可靠性等都需要自己实现；非实时，从建索引到可以搜索中间有一个时间延迟，而当前的“近实时”(Lucene Near Real Time search)搜索方案的可扩展性有待进一步完善 Solr Solr是一个企业级的高性能、采用Java开发，基于Lucene的全文搜索服务器。 文档通过Http利用XML加到一个搜索集合中。 查询该集合也是通过 http收到一个XML/JSON响应来实现。它的主要特性包括：高效、灵活的缓存功能，垂直搜索功能，高亮显示搜索结果，通过索引复制来提高可用性，提 供一套强大Data Schema来定义字段，类型和设置文本分析，提供基于Web的管理界面等。 优点： Solr有一个更大、更成熟的用户、开发和贡献者社区 支持添加多种格式的索引，如：HTML、PDF、微软 Office 系列软件格式以及 JSON、XML、CSV 等纯文本格式 Solr比较成熟、稳定 不考虑建索引的同时进行搜索，速度更快 缺点： 建立索引时，搜索效率下降，实时索引搜索效率不高 Sphinx Sphinx一个基于SQL的全文检索引擎，特别为一些脚本语言（PHP,Python，Perl，Ruby）设计搜索API接口。 Sphinx是一个用C++语言写的开源搜索引擎，也是现在比较主流的搜索引擎之一，在建立索引的时间方面比Lucene快50%，但是索引文件比Lucene要大一倍，因此Sphinx在索引的建立方面是空间换取时间的策略，在检索速度上，和lucene相差不大，但检索精准度方面Lucene要优于Sphinx，另外在加入中文分词引擎难度方面，Lucene要优于Sphinx.其中Sphinx支持实时搜索，使用起来比较简单方便. Sphinx可以非常容易的与SQL数据库和脚本语言集成。当前系统内置MySQL和PostgreSQL 数据库数据源的支持，也支持从标准输入读取特定格式 的XML数据。通过修改源代码，用户可以自行增加新的数据源（例如：其他类型的DBMS 的原生支持） Sphinx的特点： 高速的建立索引(在当代CPU上，峰值性能可达到10 MB/秒) 高性能的搜索(在2 – 4GB 的文本数据上，平均每次检索响应时间小于0.1秒) 可处理海量数据(目前已知可以处理超过100 GB的文本数据, 在单一CPU的系统上可 处理100 M 文档) 提供了优秀的相关度算法，基于短语相似度和统计（BM25）的复合Ranking方法 支持分布式搜索 支持短语搜索 提供文档摘要生成 可作为MySQL的存储引擎提供搜索服务 支持布尔、短语、词语相似度等多种检索模式 文档支持多个全文检索字段(最大不超过32个) 文档支持多个额外的属性信息(例如：分组信息，时间戳等) 支持断词 ElasticSearch ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。 优点： 分布式：节点对外表现对等，加入节点自动均衡 elasticsearch完全支持Apache Lucene的接近实时的搜索 各节点组成对等的网络结构，当某个节点出现故障时会自动分配其他节点代替期进行工作 横向可扩展性，如果你需要增加一台服务器，只需要做点配置，然后启动就完事了 高可用：提供复制（replica）机制，一个分片可以设置多个复制，使得某台服务器宕机的情况下，集群仍旧可以照常运行，并会把由于服务器宕机丢失的复制恢复到其它可用节点上；这点也类似于HDFS的复制机制（HDFS中默认是3份复制） 缺点： 不支持事物 相对吃内存 see also：Apache Lucene | 开源搜索引擎分类 | ElasticSearch vs Solr多维度分析对比 | Lucene：基于Java的全文检索引擎简介 欢迎斧正，that’s all ","date":"2020-08-31","objectID":"/posts/elasticsearch-02-apache-lucene/:0:4","tags":["Lucene"],"title":"Apache Lucene","uri":"/posts/elasticsearch-02-apache-lucene/"},{"categories":["ElasticSearch"],"content":"在linux 上安装 elasticsearch ","date":"2020-08-31","objectID":"/posts/elasticsearch-03-linux%E5%AE%89%E8%A3%85/:0:0","tags":["linux安装"],"title":"Linux 安装 Elasticsearch","uri":"/posts/elasticsearch-03-linux%E5%AE%89%E8%A3%85/"},{"categories":["ElasticSearch"],"content":"前言 由于elasticsearch依赖java环境，所以，我们首先要安装java jdk。 这里我们使es和kibana的版本保持一致，环境如下： centos7.3 java1.8 elasticsearch6.7.0 kibana6.7.0 ik6.7.0 另外，要检查一下防火墙是否关闭： Copyfirewall-cmd --state # 检查防火墙是否关闭 systemctl stop firewalld.service # 停止firewall systemctl disable firewalld.service # 禁止开机启动 ","date":"2020-08-31","objectID":"/posts/elasticsearch-03-linux%E5%AE%89%E8%A3%85/:0:1","tags":["linux安装"],"title":"Linux 安装 Elasticsearch","uri":"/posts/elasticsearch-03-linux%E5%AE%89%E8%A3%85/"},{"categories":["ElasticSearch"],"content":"Docker 安装 es 拉取镜像 sudo docker pull elasticsearch:7.2.0 等待下载完成， （选择docker是因为 docker比较轻便 不会出现各种各样的 java环境问题） 创建容器 # 在 Document 下创建 ElasticSearch 文件夹， 然后再创建 conf data logs 文件夹 用于放置 es的数据， 配置 ， 日志文件 mkdir ELasticSearch cd ElasticSearch mkdir conf mkdir logs mkdir data # 使用run 命令 创建 es 容器 sudo docker run -id --name=es-a -p 9200:9200 -p 9300:9300 -v /home/zhang/Documents/ElasticSearch/data/data-a:/usr/share/elasticsearch/data -v /home/zhang/Documents/ElasticSearch/config/el-a.yml:/usr/share/elasticsearch/config/elasticsearch.yml -v /home/zhang/Documents/ElasticSearch/config/jvm.options:/usr/share/elasticsearch/config/jvm.options -e \"discovery.type=single-node\" -e \"ES_JAVA_OPTS= -Xms1024m -Xmx1024m\" elasticsearch:7.2.0 sudo curl -L https://github.com/docker/compose/releases/download/1.25.0-rc2/docker-compose-Linux-x86_64 -o /usr/local/bin/docker-composesudo chmod +x /usr/local/bin/docker-compose sudo curl -L https://get.daocloud.io/docker/compose/releases/download/1.25.0-rc2/docker-compose-Linux-x86_64 -o /usr/local/bin/docker-composesudo chmod +x /usr/local/bin/docker-compose ","date":"2020-08-31","objectID":"/posts/elasticsearch-03-linux%E5%AE%89%E8%A3%85/:0:2","tags":["linux安装"],"title":"Linux 安装 Elasticsearch","uri":"/posts/elasticsearch-03-linux%E5%AE%89%E8%A3%85/"},{"categories":["ElasticSearch"],"content":"什么是ElasticSearch ","date":"2020-08-31","objectID":"/posts/elasticsearch-01-%E5%85%A5%E9%97%A8/:0:0","tags":["Es简介"],"title":"什么是ElasticSearch","uri":"/posts/elasticsearch-01-%E5%85%A5%E9%97%A8/"},{"categories":["ElasticSearch"],"content":"什么是ElasticSearch 现在，你还离得开搜索吗？无论是Google还是百度提供的搜索入口，还是项目自己的搜索，比如QQ提供的搜索入口等等，都大大的方便了我们的工作、生活。但是你有没有想过——搭建属于自己的搜索服务，应用于你的博客项目、公司项目…… 无论你想不想，都要学习！因为随着公司业务的增长，数据也爆炸性增长。对于数据的处理、日志分析，如果还采用传统的方法，这恐怕是灾难性的。所以，我们是时候学习一个先进的搜索引擎了。 Elasticsearch是一个基于Apache Lucene(TM)的开源搜索引擎。无论在开源还是专有领域，Lucene可以被认为是迄今为止最先进、性能最好的、功能最全的搜索引擎库。 但是，Lucene只是一个库。想要使用它，你必须使用Java来作为开发语言并将其直接集成到你的应用中，更糟糕的是，Lucene非常复杂，你需要深入了解检索的相关知识来理解它是如何工作的。 Elasticsearch也使用Java开发并使用Lucene作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的RESTful API来隐藏Lucene的复杂性，从而让全文搜索变得简单。 你以为这些就完了？elasticsearch除了Lucene和全文搜索，我们还可以描述它： 分布式的实时文件存储，每个字段都被索引并可被搜索 分布式的实时分析搜索引擎 可以扩展到上百台服务器，处理PB级结构化或非结构化数据。 并且，这些功能都被集成到一个服务里面，elasticsearch也提供的与其它语言的接口，其中包括： Java JavaScript Groovy .NET PHP Perl Python Ruby 以及社区贡献的更多接口 使用我们喜欢的语言通过RESTful API接口，访问9200端口，就可以与elasticsearch玩耍了。 上手elasticsearch非常容易，它提供了许多合理的缺省值，并对初学者隐藏了复杂的搜索引擎理论。它开箱即用（安装即可使用），只需很少的学习既可在生产环境中使用。 随着越学越深入，还可以利用Elasticsearch更多高级的功能，整个引擎可以很灵活地进行配置。可以根据自身需求来定制属于自己的Elasticsearch。 ","date":"2020-08-31","objectID":"/posts/elasticsearch-01-%E5%85%A5%E9%97%A8/:0:1","tags":["Es简介"],"title":"什么是ElasticSearch","uri":"/posts/elasticsearch-01-%E5%85%A5%E9%97%A8/"},{"categories":["ElasticSearch"],"content":"ElasticSearch 模糊历史 多年前，一个叫做Shay Banon的刚结婚不久的失业开发者，由于妻子要去伦敦学习厨师，他便跟着也去了。在他找工作的过程中，为了给妻子构建一个食谱的搜索引擎，他开始构建一个早期版本的Lucene。 直接基于Lucene工作会比较困难，所以Shay开始抽象Lucene代码以便Java程序员可以在应用中添加搜索功能。他发布了他的第一个开源项目，叫做“Compass”。 后来Shay找到一份工作，这份工作处在高性能和内存数据网格的分布式环境中，因此高性能的、实时的、分布式的搜索引擎也是理所当然需要的。然后他决定重写Compass库使其成为一个独立的服务叫做Elasticsearch。 第一个公开版本出现在2010年2月，在那之后Elasticsearch已经成为Github上最受欢迎的项目之一，代码贡献者超过300人。一家主营Elasticsearch的公司就此成立，他们一边提供商业支持一边开发新功能，不过Elasticsearch将永远开源且对所有人可用。 Shay的妻子依旧等待着她的食谱搜索…… ","date":"2020-08-31","objectID":"/posts/elasticsearch-01-%E5%85%A5%E9%97%A8/:0:2","tags":["Es简介"],"title":"什么是ElasticSearch","uri":"/posts/elasticsearch-01-%E5%85%A5%E9%97%A8/"},{"categories":["ElasticSearch"],"content":"ElasticSearch ： 面向文档（工作原理） 我们知道，关系型数据库以记录和行的形式存储数据，但是在elasticsearch中，是以文档的形式存储数据。 但区别在于，文档要比数据表的行更加灵活。因为文档可以是多层次的，它（文档）鼓励你将属于一个逻辑实体的数据保存在同一个文档中，而不是散落在各个表的不同行中。这样查询效率很高，因为我们无需连接其他的表，我们学习关系型数据库时，一定知道连表查询（尤其是连接多张表）是多么的费时吧！ ","date":"2020-08-31","objectID":"/posts/elasticsearch-01-%E5%85%A5%E9%97%A8/:0:3","tags":["Es简介"],"title":"什么是ElasticSearch","uri":"/posts/elasticsearch-01-%E5%85%A5%E9%97%A8/"},{"categories":["ElasticSearch"],"content":"成功案例 之前有人说，elasticsearch的缺点之一是没有成熟的案例加持，那我们就来看看elasticsearch都有哪些成熟的案例： 维基百科使用Elasticsearch来进行全文搜做并高亮显示关键词，以及提供search-as-you-type、did-you-mean等搜索建议功能。 英国卫报使用Elasticsearch来处理访客日志，以便能将公众对不同文章的反应实时地反馈给各位编辑。 StackOverflow将全文搜索与地理位置和相关信息进行结合，以提供more-like-this相关问题的展现。 GitHub使用Elasticsearch来检索超过1300亿行代码，可以参考A Whole New Code Search 每天，Goldman Sachs使用它来处理5TB数据的索引，还有很多投行使用它来分析股票市场的变动。 苏宁在大数据平台使用es存储600TB数据，集群规模包括：搭建超过500+物理机，30万shards，80000index。参考 腾讯在日志实时分析中采用es，处理高并发100W/S，PE级数据。 更多参考 所以，elasticsearch可以灵活的应用于我们的项目中。 ","date":"2020-08-31","objectID":"/posts/elasticsearch-01-%E5%85%A5%E9%97%A8/:0:4","tags":["Es简介"],"title":"什么是ElasticSearch","uri":"/posts/elasticsearch-01-%E5%85%A5%E9%97%A8/"},{"categories":["ElasticSearch"],"content":"如何学elasticsearch 除了万能的百度和Google 之外，我们还有一些其他的学习途径： elasticsearch官方文档：这个比较好点，可以多多参考 elasticsearch博客：这个吧，看看就行 elasticsearch社区：社区还是很好的 elasticsearch视频：包括入门视频什么的 elasticsearch实战：该书籍的质量还是不错的。 elasticsearch权威指南：同样的，这个也不错。 ","date":"2020-08-31","objectID":"/posts/elasticsearch-01-%E5%85%A5%E9%97%A8/:0:5","tags":["Es简介"],"title":"什么是ElasticSearch","uri":"/posts/elasticsearch-01-%E5%85%A5%E9%97%A8/"},{"categories":["ElasticSearch"],"content":"ElasticSearch 能处理的数据量 一个很好地问题，不幸的是，单一索引的极限取决于存储索引的硬件、索引的设计、如何处理数据以及你为索引备份了多少副本。 通常来说，一个Lucene索引（也就是一个elasticsearch分片）不能处理多于21亿篇文档，或者多于2740亿的唯一词条。但达到这个极限之前，我们可能就没有足够的磁盘空间了！ 当然，一个分片如何很大的话，读写性能将会变得非常差。 扯了半天的淡，让我们开始一个灵活的学习之旅吧。 see also： 入门指南 | Elasticsearch Clients | 搜索引擎选择： Elasticsearch与Solr | Elasticsearch 基本介绍及其与 Python 的对接实现 | Elasticsearch权威指南中文版（截止本博客发表，还没有翻译完…….） 欢迎斧正，that’s all ","date":"2020-08-31","objectID":"/posts/elasticsearch-01-%E5%85%A5%E9%97%A8/:0:6","tags":["Es简介"],"title":"什么是ElasticSearch","uri":"/posts/elasticsearch-01-%E5%85%A5%E9%97%A8/"},{"categories":["爬虫"],"content":"爬虫 Scrapy高级 使用 ","date":"2017-07-09","objectID":"/posts/spider-7%E5%8A%A8%E4%BD%9C%E9%93%BE%E8%87%AA%E5%8A%A8%E7%99%BB%E5%BD%9512306/:0:0","tags":["爬虫","scrapy"],"title":"动作链","uri":"/posts/spider-7%E5%8A%A8%E4%BD%9C%E9%93%BE%E8%87%AA%E5%8A%A8%E7%99%BB%E5%BD%9512306/"},{"categories":["爬虫"],"content":"动作链 from selenium import webdriver from selenium.webdriver import ActionChains import time bro=webdriver.Chrome(executable_path='./chromedriver') bro.get('https://www.runoob.com/try/try.php?filename=jqueryui-api-droppable') bro.implicitly_wait(10) #切换frame（很少） bro.switch_to.frame('iframeResult') div=bro.find_element_by_xpath('//*[@id=\"draggable\"]') # 使用动作链 #得到一个动作练对象 action=ActionChains(bro) # 使用动作链 #点击并且夯住 action.click_and_hold(div) # 直接把上面的div移动到某个元素上 # action.move_to_element(元素控件) # 移动x坐标，y坐标 # 三种移动方式 # action.move_by_offset() # 通过坐标 # action.move_to_element() # 到另一个标签 # action.move_to_element_with_offset() # 到另一个标签，再偏移一部分 for i in range(5): action.move_by_offset(10,10) # 直接把上面的div移动到某个元素上的某个位置 # action.move_to_element_with_offset() # 调用它，会动起来 action.perform() time.sleep(1) #释放动作链 action.release() time.sleep(5) bro.close() ","date":"2017-07-09","objectID":"/posts/spider-7%E5%8A%A8%E4%BD%9C%E9%93%BE%E8%87%AA%E5%8A%A8%E7%99%BB%E5%BD%9512306/:1:0","tags":["爬虫","scrapy"],"title":"动作链","uri":"/posts/spider-7%E5%8A%A8%E4%BD%9C%E9%93%BE%E8%87%AA%E5%8A%A8%E7%99%BB%E5%BD%9512306/"},{"categories":["爬虫"],"content":"爬虫 Scrapy高级 使用 ","date":"2017-07-08","objectID":"/posts/spider-6scrapy%E9%AB%98%E7%BA%A7/:0:0","tags":["爬虫","scrapy"],"title":"Scrapy高级","uri":"/posts/spider-6scrapy%E9%AB%98%E7%BA%A7/"},{"categories":["爬虫"],"content":"scrapy 请求传参 # 1 放 ：yield Request(url,callback=self.parser_detail,meta={'item':item}) # 2 取：response.meta.get('item') # 3 指定解析函数 callback=self.解析函数. class CnblogSpider(scrapy.Spider): name = 'cnblog' # allowed_domains = ['https://www.cnblogs.com/'] start_urls = ['https://www.cnblogs.com/sitehome/p/1'] num = 0 # post_list \u003e div:nth-child(19) \u003e div def parse(self, response): article_list = response.css(\".post_item_body\") for article in article_list: item = CrawlCnblogsItem() title = article.css(\".titlelnk::text\").extract_first() url = article.css(\".titlelnk::attr(href)\").extract_first() desc = article.css(\"p::text\").extract()[-1] author = article.css(\".post_item_foot\u003ea::text\").extract_first() comment = article.xpath(\".//*[contains(@class,'post_item_foot')]/span[1]/a/text()\").extract_first() view = article.xpath(\".//*[contains(@class,'post_item_foot')]/span[2]/a/text()\").extract_first() item[\"title\"] = title.strip() item[\"url\"] = url item[\"desc\"] = desc item[\"author\"] = author.strip() item[\"comment\"] = comment.strip() item[\"view\"] = view.strip() yield item nextpage = f'https://www.cnblogs.com{response.css(\".pager\u003e a:last-child::attr(href)\").extract_first()}' if nextpage and (self.num \u003c 10): yield Request(url=nextpage, callback=self.parse) ","date":"2017-07-08","objectID":"/posts/spider-6scrapy%E9%AB%98%E7%BA%A7/:1:0","tags":["爬虫","scrapy"],"title":"Scrapy高级","uri":"/posts/spider-6scrapy%E9%AB%98%E7%BA%A7/"},{"categories":["爬虫"],"content":"提高爬取效率 - 在配置文件中进行相关的配置即可:(默认还有一套setting) #1 增加并发： 默认scrapy开启的并发线程为32个，可以适当进行增加。在settings配置文件中修改CONCURRENT_REQUESTS = 100值为100,并发设置成了为100。 #2 提高日志级别： 在运行scrapy时，会有大量日志信息的输出，为了减少CPU的使用率。可以设置log输出信息为INFO或者ERROR即可。在配置文件中编写：LOG_LEVEL = ‘INFO’ # 3 禁止cookie： 如果不是真的需要cookie，则在scrapy爬取数据时可以禁止cookie从而减少CPU的使用率，提升爬取效率。在配置文件中编写：COOKIES_ENABLED = False # 4禁止重试： 对失败的HTTP进行重新请求（重试）会减慢爬取速度，因此可以禁止重试。在配置文件中编写：RETRY_ENABLED = False # 5 减少下载超时： 如果对一个非常慢的链接进行爬取，减少下载超时可以能让卡住的链接快速被放弃，从而提升效率。在配置文件中进行编写：DOWNLOAD_TIMEOUT = 10 超时时间为10s ","date":"2017-07-08","objectID":"/posts/spider-6scrapy%E9%AB%98%E7%BA%A7/:2:0","tags":["爬虫","scrapy"],"title":"Scrapy高级","uri":"/posts/spider-6scrapy%E9%AB%98%E7%BA%A7/"},{"categories":["爬虫"],"content":"下载中间件 # 2大中间件：下载中间件，爬虫中间件 # 1 写在middlewares.py中（名字随便命名） # 2 配置生效（） SPIDER_MIDDLEWARES = { 'cnblogs_crawl.middlewares.CnblogsCrawlSpiderMiddleware': 543, } DOWNLOADER_MIDDLEWARES = { 'cnblogs_crawl.middlewares.CnblogsCrawlDownloaderMiddleware': 543, } # 2 下载中间件 -process_request：（请求去，走） # - return None: 继续处理当次请求，进入下一个中间件 # - return Response： 当次请求结束，把Response丢给引擎处理（可以自己爬，包装成Response） # - return Request ： 相当于把Request重新给了引擎，引擎再去做调度 # - 抛异常：执行process_exception -process_response：（请求回来，走） # - return a Response object ：继续处理当次Response，继续走后续的中间件 # - return a Request object：重新给引擎做调度 # - or raise IgnoreRequest ：process_exception -process_exception：（出异常，走） # - return None: continue processing this exception # - return a Response object: stops process_exception() chain ：停止异常处理链，给引擎（给爬虫） # - return a Request object: stops process_exception() chain ：停止异常处理链，给引擎（重新调度） ","date":"2017-07-08","objectID":"/posts/spider-6scrapy%E9%AB%98%E7%BA%A7/:3:0","tags":["爬虫","scrapy"],"title":"Scrapy高级","uri":"/posts/spider-6scrapy%E9%AB%98%E7%BA%A7/"},{"categories":["爬虫"],"content":"集成 selenium # 在爬虫已启动，就打开一个chrom浏览器，以后都用这一个浏览器来爬数据 # 1 在爬虫中创建bro对象 bro = webdriver.Chrome(executable_path='./chromedriver') # 2 中间件中使用： spider.bro.get(request.url) text=spider.bro.page_source response=HtmlResponse(url=request.url,status=200,body=text.encode('utf-8')) return response # 3 关闭，在爬虫中 def close(self, reason): self.bro.close() ","date":"2017-07-08","objectID":"/posts/spider-6scrapy%E9%AB%98%E7%BA%A7/:4:0","tags":["爬虫","scrapy"],"title":"Scrapy高级","uri":"/posts/spider-6scrapy%E9%AB%98%E7%BA%A7/"},{"categories":["爬虫"],"content":"fack-useragent # 请求头中的user-agent list=['',''] # pip3 install fake-useragent # https://github.com/hellysmile/fake-useragent from fake_useragent import UserAgent ua=UserAgent(verify_ssl=False) print(ua.random) ","date":"2017-07-08","objectID":"/posts/spider-6scrapy%E9%AB%98%E7%BA%A7/:5:0","tags":["爬虫","scrapy"],"title":"Scrapy高级","uri":"/posts/spider-6scrapy%E9%AB%98%E7%BA%A7/"},{"categories":["爬虫"],"content":"去重源码分析 # 去重源码分析 # from scrapy.core.scheduler import Scheduler # Scheduler下：def enqueue_request(self, request)方法判断是否去重 if not request.dont_filter and self.df.request_seen(request): Requests对象，RFPDupeFilter对象 # 如果要自己写一个去重类 -写一个类，继承BaseDupeFilter类 -重写def request_seen(self, request): -在setting中配置：DUPEFILTER_CLASS = '项目名.dup.UrlFilter' # scrapy起始爬取的地址 def start_requests(self): for url in self.start_urls: yield Request(url) 增量爬取 -增量爬取（100链接，150个链接） -已经爬过的，放到某个位置（mysql，redis中：集合） -如果用默认的，爬过的地址，放在内存中，只要项目一重启，就没了，它也不知道我爬过那个了，所以要自己重写去重方案 -你写的去重方案，占得内存空间更小 -bitmap方案 -BloomFilter布隆过滤器 from scrapy.http import Request from scrapy.utils.request import request_fingerprint # 这种网址是一个 requests1=Request(url='https://www.baidu.com?name=lqz\u0026age=19') requests2=Request(url='https://www.baidu.com?age=18\u0026name=lqz') ret1=request_fingerprint(requests1) ret2=request_fingerprint(requests2) print(ret1) print(ret2) # bitmap去重 一个小格表示一个连接地址 32个连接，一个比特位来存一个地址 # https://www.baidu.com?age=18\u0026name=lqz ---》44 # https://www.baidu.com?age=19\u0026name=lqz ---》89 # c2c73dfccf73bf175b903c82b06a31bc7831b545假设它占4个bytes，4*8=32个比特位 # 存一个地址，占32个比特位 # 10个地址，占320个比特位 #计算机计量单位 # 比特位：只能存0和1 # 8个比特位是一个bytes # 1024bytes=1kb # 1024kb=1m # 1024m=1g # 布隆过滤器：原理和python中如何使用 def request_seen(self, request): # 把request对象传入request_fingerprint得到一个值：aefasdfeasd # 把request对象，唯一生成一个字符串 fp = self.request_fingerprint(request) #判断fp，是否在集合中，在集合中，表示已经爬过，return True，他就不会再爬了 if fp in self.fingerprints: return True # 如果不在集合中，放到集合中 self.fingerprints.add(fp) if self.file: self.file.write(fp + os.linesep) ","date":"2017-07-08","objectID":"/posts/spider-6scrapy%E9%AB%98%E7%BA%A7/:6:0","tags":["爬虫","scrapy"],"title":"Scrapy高级","uri":"/posts/spider-6scrapy%E9%AB%98%E7%BA%A7/"},{"categories":["爬虫"],"content":"分布式爬虫 # 1 安装pip3 install scrapy-redis # 1 原来的爬虫继承 from scrapy_redis.spiders import RedisSpider class CnblogsSpider(RedisSpider): #start_urls = ['http://www.cnblogs.com/'] redis_key = 'myspider:start_urls' # 2 在setting中配置 SCHEDULER = \"scrapy_redis.scheduler.Scheduler\" DUPEFILTER_CLASS = \"scrapy_redis.dupefilter.RFPDupeFilter\" ITEM_PIPELINES = { 'scrapy_redis.pipelines.RedisPipeline': 300 } # 3 多台机器上启动scrapy # 4 向reids中发送起始url lpush myspider:start_urls https://www.cnblogs.com ","date":"2017-07-08","objectID":"/posts/spider-6scrapy%E9%AB%98%E7%BA%A7/:7:0","tags":["爬虫","scrapy"],"title":"Scrapy高级","uri":"/posts/spider-6scrapy%E9%AB%98%E7%BA%A7/"},{"categories":["爬虫"],"content":"爬虫 Scrapy框架 使用 ","date":"2017-07-07","objectID":"/posts/spider-5scrapy%E6%A1%86%E6%9E%B6/:0:0","tags":["爬虫","scrapy"],"title":"Scrapy框架","uri":"/posts/spider-5scrapy%E6%A1%86%E6%9E%B6/"},{"categories":["爬虫"],"content":"1 Scrapy 介绍/架构 Scrapy一个开源和协作的框架，其最初是为了页面抓取 (更确切来说, 网络抓取 )所设计的，使用它可以以快速、简单、可扩展的方式从网站中提取所需的数据。但目前Scrapy的用途十分广泛，可用于如数据挖掘、监测和自动化测试等领域，也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫。 Scrapy 是基于twisted框架开发而来，twisted是一个流行的事件驱动的python网络框架。因此Scrapy使用了一种非阻塞（又名异步）的代码来实现并发。整体架构大致如下 Components： 引擎(EGINE) 引擎负责控制系统所有组件之间的数据流，并在某些动作发生时触发事件。有关详细信息，请参见上面的数据流部分。 调度器(SCHEDULER) 用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL的优先级队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址 下载器(DOWLOADER) 用于下载网页内容, 并将网页内容返回给EGINE，下载器是建立在twisted这个高效的异步模型上的 爬虫(SPIDERS) SPIDERS是开发人员自定义的类，用来解析responses，并且提取items，或者发送新的请求 项目管道(ITEM PIPLINES) 在items被提取后负责处理它们，主要包括清理、验证、持久化（比如存到数据库）等操作 下载器中间件(Downloader Middlewares) 位于Scrapy引擎和下载器之间，主要用来处理从EGINE传到DOWLOADER的请求request，已经从DOWNLOADER传到EGINE的响应response，你可用该中间件做以下几件事 process a request just before it is sent to the Downloader (i.e. right before Scrapy sends the request to the website); change received response before passing it to a spider; send a new Request instead of passing received response to a spider; pass response to a spider without fetching a web page; silently drop some requests. 爬虫中间件(Spider Middlewares) 位于EGINE和SPIDERS之间，主要工作是处理SPIDERS的输入（即responses）和输出（即requests） 官网链接：https://docs.scrapy.org/en/latest/topics/architecture.html ","date":"2017-07-07","objectID":"/posts/spider-5scrapy%E6%A1%86%E6%9E%B6/:1:0","tags":["爬虫","scrapy"],"title":"Scrapy框架","uri":"/posts/spider-5scrapy%E6%A1%86%E6%9E%B6/"},{"categories":["爬虫"],"content":"2 Scripy框架的安装和启动 安装 #Windows平台 1、pip3 install wheel #安装后，便支持通过wheel文件安装软件，wheel文件官网：https://www.lfd.uci.edu/~gohlke/pythonlibs 3、pip3 install lxml 4、pip3 install pyopenssl 5、下载并安装pywin32：https://sourceforge.net/projects/pywin32/files/pywin32/ 6、下载twisted的wheel文件：http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted 7、执行pip3 install 下载目录\\Twisted-17.9.0-cp36-cp36m-win_amd64.whl 8、pip3 install scrapy #Linux和 mac平台 1、pip3 install scrapy 创建 scrapy startproject #创建项目 scrapy genspider #创建爬虫程序 启动 scrapy crawl 爬虫名字 # 启动爬虫 scrapy crawl 爬虫名字 --nolog # 不打印日志启动 从文件启动 from scrapy.cmdline import execute # execute(['scrapy','crawl','chouti','--nolog']) execute(['scrapy','crawl','chouti']) ","date":"2017-07-07","objectID":"/posts/spider-5scrapy%E6%A1%86%E6%9E%B6/:2:0","tags":["爬虫","scrapy"],"title":"Scrapy框架","uri":"/posts/spider-5scrapy%E6%A1%86%E6%9E%B6/"},{"categories":["爬虫"],"content":"3 配置文件和目录介绍 目录介绍 -crawl_chouti # 项目名 -crawl_chouti # 跟项目一个名，文件夹 -spiders # spiders：放着爬虫 genspider生成的爬虫，都放在这下面 -__init__.py -chouti.py # 抽屉爬虫 -cnblogs.py # cnblogs 爬虫 -items.py # 对比django中的models.py文件 ,写一个个的模型类 -middlewares.py # 中间件（爬虫中间件，下载中间件），中间件写在这 -pipelines.py # 写持久化的地方（持久化到文件，mysql，redis，mongodb） -settings.py # 配置文件 -scrapy.cfg # 不用关注，上线相关的 配置文件 # 配置文件 ROBOTSTXT_OBEY = False # 是否遵循爬虫协议，强行运行 USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36' # 请求头中的ua LOG_LEVEL='ERROR' # 这样配置，程序错误信息才会打印， #启动爬虫直接 scrapy crawl 爬虫名 就没有日志输出 # scrapy crawl 爬虫名 --nolog 爬虫文件 class ChoutiSpider(scrapy.Spider): name = 'chouti' # 爬虫名字 allowed_domains = ['https://dig.chouti.com/'] # 允许爬取的域 start_urls = ['https://dig.chouti.com/'] # 起始爬取的位置，爬虫一启动，会先向它发请求 def parse(self, response): # 解析，请求回来，自动执行parser，在这个方法中做解析 print('---------------------------',response) 数据解析 使用bs4 # 1 解析，可以使用bs4解析 from bs4 import BeautifulSoup soup=BeautifulSoup(response.text,'lxml') soup.find_all() 使用内置解析器 response.css response.xpath # 解析 # 所有用css或者xpath选择出来的都放在列表中 # 取第一个:extract_first() # 取出所有extract() # css选择器取文本和属性： response.css(\".link-title::text\") response.css(\".link-title::attr(href)\") # xpath选择器取文本和属性 response.xpath('.//a[contains(@class,\"link-title\")/text()]') response.xpath('//a[contains(@class,\"link-title\")/@href]') ","date":"2017-07-07","objectID":"/posts/spider-5scrapy%E6%A1%86%E6%9E%B6/:3:0","tags":["爬虫","scrapy"],"title":"Scrapy框架","uri":"/posts/spider-5scrapy%E6%A1%86%E6%9E%B6/"},{"categories":["爬虫"],"content":"4 数据持久化 # 方式一 -1 parser解析函数，return 列表，列表套字典 -2 scrapy crawl chouti -o aa.json (支持：('json', 'jsonlines', 'jl', 'csv', 'xml', 'marshal', 'pickle') # 方式二 pipline的方式（管道） -1 在items.py中创建模型类 -2 在爬虫中chouti.py，引入，把解析的数据放到item对象中（要用中括号） -3 yield item对象 -4 配置文件配置管道 ITEM_PIPELINES = { # 数字表示优先级（数字越小，优先级越大） 'crawl_chouti.pipelines.CrawlChoutiPipeline': 300, 'crawl_chouti.pipelines.CrawlChoutiRedisPipeline': 301， } -5 pipline.py中写持久化的类 # 在保存数据之前执行的函数 -spider_open # 在执行数据持久化之后执行的函数 -spider_close -process_item（在这写保存到哪） [toc] ","date":"2017-07-07","objectID":"/posts/spider-5scrapy%E6%A1%86%E6%9E%B6/:4:0","tags":["爬虫","scrapy"],"title":"Scrapy框架","uri":"/posts/spider-5scrapy%E6%A1%86%E6%9E%B6/"},{"categories":["爬虫"],"content":"爬虫 selenium 模块使用 ","date":"2017-07-06","objectID":"/posts/spider-4selenium/:0:0","tags":["爬虫","代理池"],"title":"selenium","uri":"/posts/spider-4selenium/"},{"categories":["爬虫"],"content":"介绍 selenium最初是一个自动化测试工具,而爬虫中使用它主要是为了解决requests无法直接执行JavaScript代码的问题 可以操作浏览器(火狐，谷歌（建议你用谷歌），ie)，模拟人的行为（人可以干啥，代码控制就可以干啥） ","date":"2017-07-06","objectID":"/posts/spider-4selenium/:1:0","tags":["爬虫","代理池"],"title":"selenium","uri":"/posts/spider-4selenium/"},{"categories":["爬虫"],"content":"selenium的简单使用 # pip3 install selenium # 1 基本使用 from selenium import webdriver # import time # # 得到 一个谷歌浏览器对象 # # 代码不能直接操作浏览器，需要有一个浏览器驱动（配套的） # # 下载谷歌浏览器驱动：http://npm.taobao.org/mirrors/chromedriver/ # # 谷歌浏览器驱动要跟谷歌版本对应 # # http://npm.taobao.org/mirrors/chromedriver/80.0.3987.106/ ：80.0.3987.149（正式版本） # # 指定一下驱动的位置（相对路径/绝对路径） # bro=webdriver.Chrome(executable_path='./chromedriver') # # bro.get(\"https://www.baidu.com\") # # # 页面内容 # # ret.text 相当于它，可以使用bs4解析数据，或者用selenium自带的解析器解析 # print(bro.page_source) # time.sleep(5) # bro.close() ","date":"2017-07-06","objectID":"/posts/spider-4selenium/:2:0","tags":["爬虫","代理池"],"title":"selenium","uri":"/posts/spider-4selenium/"},{"categories":["爬虫"],"content":"selenium的高级用法 常用方法 bro=webdriver.Chrome(executable_path='./chromedriver') bro.get(\"https://www.baidu.com\") 解析器 # 1、find_element_by_id # id找 # 2、find_element_by_link_text # a标签上的文字找 # 3、find_element_by_partial_link_text # a标签上的文字模糊 # 4、find_element_by_tag_name # 根据标签名字找 # 5、find_element_by_class_name # 根据类名字找 # 6、find_element_by_name # name='xx' 根据name属性找 # 7、find_element_by_css_selector # css选择器找 # 8、find_element_by_xpath #xpath选择器找 在输入框中输入美女（自带的解析器，查找输入框空间） # //*[@id=\"kw\"] # input_search=bro.find_element_by_xpath('//*[@id=\"kw\"]') input_search=bro.find_element_by_css_selector('#kw') 写文字 input_search.send_keys(\"美女\") 查找搜索按钮 enter=bro.find_element_by_id('su') 点击按钮 enter.click() 关闭浏览器 bro.close() 小案例 import time bro=webdriver.Chrome(executable_path='./chromedriver') bro.get(\"https://www.baidu.com\") # # 隐士等待(最多等待10s) # 只有控件没有加载出来，才会等，控件一旦加载出来，直接就取到 bro.implicitly_wait(10) submit_button=bro.find_element_by_link_text('登录') submit_button.click() user_button=bro.find_element_by_id('TANGRAM__PSP_10__footerULoginBtn') user_button.click() user_input=bro.find_element_by_id('TANGRAM__PSP_10__userName') user_input.send_keys(\"ssssss@qq.com\") pwd_input=bro.find_element_by_id('TANGRAM__PSP_10__password') pwd_input.send_keys(\"123456\") submit_input=bro.find_element_by_id('TANGRAM__PSP_10__submit') submit_input.click() time.sleep(5) bro.close() 获取 cookie # 登陆之后，拿到cookie：就可以自己搭建cookie池（requests模块发请求，携带者cookie） import time bro=webdriver.Chrome(executable_path='./chromedriver') bro.get(\"https://www.baidu.com\") print(bro.get_cookies()) bro.close() # #搭建cookie池和代理池的作用是什么？封ip ，封账号（弄一堆小号，一堆cookie） 无界面浏览器 from selenium.webdriver.chrome.options import Options chrome_options = Options() chrome_options.add_argument('window-size=1920x3000') #指定浏览器分辨率 chrome_options.add_argument('--disable-gpu') #谷歌文档提到需要加上这个属性来规避bug chrome_options.add_argument('--hide-scrollbars') #隐藏滚动条, 应对一些特殊页面 chrome_options.add_argument('blink-settings=imagesEnabled=false') #不加载图片, 提升速度 chrome_options.add_argument('--headless') #浏览器不提供可视化页面. linux下如果系统不支持可视化不加这条会启动失败 bro=webdriver.Chrome(executable_path='./chromedriver',options=chrome_options) bro.get(\"https://www.baidu.com\") print(bro.get_cookies()) bro.close 获取标签属性(重点) print(tag.get_attribute('src')) print(tag.get_attribute('href')) 获取标签文本(重点) print(tag.text) 获取标签ID，位置，名称，大小（了解） print(tag.id) print(tag.location) print(tag.tag_name) print(tag.size) 显示等待与隐式等待 # 隐士等待(最多等待10s) bro.implicitly_wait(10) # 只有控件没有加载出来，才会等，控件一旦加载出来，直接就取到 # 显示等待（每个控件，都要写等待），不要使用 元素交互操作 # 点击click，清空clear，输入文字send_keys 执行js import time bro=webdriver.Chrome(executable_path='./chromedriver') # bro.get(\"https://www.cnblogs.com\") # 执行js代码 # bro.execute_script('alert(1)') # window.scrollTo(0,document.body.scrollHeight) # 使页面滚动到最低层 bro.execute_script('window.scrollTo(0,document.body.scrollHeight)') time.sleep(5) bro.close() 模拟浏览器的前进后头 import time bro=webdriver.Chrome(executable_path='./chromedriver') bro.get(\"https://www.cnblogs.com\") time.sleep(1) bro.get(\"https://www.baidu.com\") time.sleep(1) bro.get(\"https://www.jd.com\") #退到上一个 bro.back() time.sleep(1) # 前进一下 bro.forward() time.sleep(5) bro.close() 选项卡管理 import time from selenium import webdriver # browser=webdriver.Chrome(executable_path='./chromedriver') browser.get('https://www.baidu.com') browser.execute_script('window.open()') # 本质上是执行的js代码 print(browser.window_handles) #获取所有的选项卡 browser.switch_to_window(browser.window_handles[1]) browser.get('https://www.taobao.com') time.sleep(2) browser.switch_to_window(browser.window_handles[0]) browser.get('https://www.sina.com.cn') browser.close() 异常处理 from selenium import webdriver from selenium.common.exceptions import TimeoutException,NoSuchElementException,NoSuchFrameException try: browser=webdriver.Chrome(executable_path='./chromedriver') browser.get('http://www.baidu.com') browser.find_element_by_id(\"xxx\") # except Exception as e: # print(e) finally: browser.close() ","date":"2017-07-06","objectID":"/posts/spider-4selenium/:3:0","tags":["爬虫","代理池"],"title":"selenium","uri":"/posts/spider-4selenium/"},{"categories":["爬虫"],"content":"小案例 ######## # 爬取京东商品信息 ####### from selenium import webdriver import time from selenium.webdriver.common.keys import Keys bro=webdriver.Chrome(executable_path='./chromedriver') def get_goods(bro): # find_elements_by_class_name 找所有 # find_element_by_class_name 找一个 li_list=bro.find_elements_by_class_name('gl-item') # ul_list=bro.find_elements_by_css_selector('.gl-item') for li in li_list: url=li.find_element_by_css_selector('.p-img\u003ea').get_attribute('href') url_img=li.find_element_by_css_selector('.p-img img').get_attribute(\"src\") if not url_img: url_img='https:'+li.find_element_by_css_selector('.p-img img').get_attribute(\"data-lazy-img\") price=li.find_element_by_css_selector('.p-price i').text name=li.find_element_by_css_selector('.p-name em').text commit=li.find_element_by_css_selector('.p-commit a').text print(''' 商品名字：%s 商品价格：%s 商品图片地址：%s 商品地址：%s 商品评论数：%s '''%(name,price,url,url_img,commit)) #查找下一页按钮 next=bro.find_element_by_partial_link_text('下一页') time.sleep(1) next.click() #继续抓取下一页 get_goods(bro) try: bro.get('https://www.jd.com') #隐士等待 bro.implicitly_wait(10) input_search=bro.find_element_by_id('key') input_search.send_keys(\"精品内衣\") #模拟键盘操作(模拟键盘敲回车) input_search.send_keys(Keys.ENTER) get_goods(bro) except Exception as e: print(e) finally: bro.close() [toc] ","date":"2017-07-06","objectID":"/posts/spider-4selenium/:3:1","tags":["爬虫","代理池"],"title":"selenium","uri":"/posts/spider-4selenium/"},{"categories":["爬虫"],"content":"爬虫 代理池 模块使用 ","date":"2017-07-06","objectID":"/posts/spider-3%E4%BB%A3%E7%90%86%E6%B1%A0/:0:0","tags":["爬虫","代理池"],"title":"代理池","uri":"/posts/spider-3%E4%BB%A3%E7%90%86%E6%B1%A0/"},{"categories":["爬虫"],"content":"免费代理池 # https://github.com/jhao104/proxy_pool # 收费的：提供给你一个接口，每掉一次这个接口，获得一个代理 # 免费：用爬虫爬取，免费代理，放到我的库中，flask，django搭一个服务（删除代理，自动测试代理可用性），每次发一个请求，获取一个代理 # 带你配置 # 1 下载，解压，用pycharm打开 # 2 安装依赖 pip install -r requirements.txt # 3 配置Config/setting.py: DB_TYPE = getenv('db_type', 'redis').upper() DB_HOST = getenv('db_host', '127.0.0.1') DB_PORT = getenv('db_port', 6379) DB_PASSWORD = getenv('db_password', '') # 4 本地启动redis-server # 5 可以在cli目录下通过ProxyPool.py -python proxyPool.py schedule :调度程序，他会取自动爬取免费代理 -python proxyPool.py webserver:启动api服务，把flask启动起来 ","date":"2017-07-06","objectID":"/posts/spider-3%E4%BB%A3%E7%90%86%E6%B1%A0/:1:0","tags":["爬虫","代理池"],"title":"代理池","uri":"/posts/spider-3%E4%BB%A3%E7%90%86%E6%B1%A0/"},{"categories":["爬虫"],"content":"验证码破解 # 1 简单验证码，字母，数字 # 2 高级的，选择，你好，12306选择乒乓球，滑动验证（极验） # 打码平台（自动破解验证码，需要花钱）云打码，超级鹰（12306） http://www.yundama.com/ http://www.chaojiying.com/ # 注册账号，（充钱）把demo下载下来，运行即可 ","date":"2017-07-06","objectID":"/posts/spider-3%E4%BB%A3%E7%90%86%E6%B1%A0/:2:0","tags":["爬虫","代理池"],"title":"代理池","uri":"/posts/spider-3%E4%BB%A3%E7%90%86%E6%B1%A0/"},{"categories":["爬虫"],"content":"爬取段子发送给女朋友 ##### # 1 爬取糗事百科，微信自动发送 ##### # https://www.qiushibaike.com/text/ # https://www.qiushibaike.com/text/page/1/ import requests from bs4 import BeautifulSoup ret=requests.get('https://www.qiushibaike.com/text/page/1/') # print(ret.text) ll=[] soup=BeautifulSoup(ret.text,\"lxml\") article_list=soup.find_all(name='div',id=True,class_='article') for article in article_list: content=article.find(name='div',class_='content').span.text # content=article.find(name='div',class_='content').text # content=article.find(class_='content').text # print(content) # 入库 #我们放到列表中 ll.append(content) print(ll) # 微信自动发消息 # wxpy：实现了web微信的接口 # pip3 install wxpy from wxpy import * # 实例化得到一个对象，微信机器人对象 import random bot=Bot(cache_path=True) @bot.register() # 接收从指定好友发来的消息，发送者即recv_msg.sender为指定好友girl_friend def recv_send_msg(recv_msg): print('收到的消息：',recv_msg.text) # recv_msg.text取得文本 return random.choice(ll) embed() ","date":"2017-07-06","objectID":"/posts/spider-3%E4%BB%A3%E7%90%86%E6%B1%A0/:3:0","tags":["爬虫","代理池"],"title":"代理池","uri":"/posts/spider-3%E4%BB%A3%E7%90%86%E6%B1%A0/"},{"categories":["爬虫"],"content":"爬虫 bs4 模块使用 ","date":"2017-07-04","objectID":"/posts/spider-2%E7%88%AC%E8%99%AB%E9%AB%98%E7%BA%A7/:0:0","tags":["爬虫","bs4"],"title":"爬虫入门/bs4","uri":"/posts/spider-2%E7%88%AC%E8%99%AB%E9%AB%98%E7%BA%A7/"},{"categories":["爬虫"],"content":"bs4的使用 从html或者xml中提取数据的python库，修改xml # 安装 pip3 install beautifulsoup4 # 使用 from bs4 import BeautifulSoup # 实例化得到对象，传入要解析的文本，解析器 # html.parser内置解析器，速度稍微慢一些，但是不需要装第三方模块 # lxml：速度快一些，但是需要安装 pip3 install lxml soup=BeautifulSoup(ret.text,'html.parser') # soup=BeautifulSaoup(open('a.html','r')) # find（找到的第一个） # find_all(找到的所有) 遍历文档树 from bs4 import BeautifulSoup html_doc = \"\"\" \u003chtml\u003e\u003chead\u003e\u003ctitle\u003eThe Dormouse's story\u003c/title\u003e\u003c/head\u003e \u003cbody\u003e \u003cp class=\"title\"id=\"id_p\"\u003e\u003cb\u003eThe Dormouse's story\u003c/b\u003e\u003c/p\u003e \u003cp class=\"story\"\u003eOnce upon a time there were three little sisters; and their names were \u003ca href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"\u003eElsie\u003c/a\u003e, \u003ca href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"\u003eLacie\u003c/a\u003e and \u003ca href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"\u003eTillie\u003c/a\u003e; and they lived at the bottom of a well.\u003c/p\u003e \u003cp class=\"story\"\u003e...\u003c/p\u003e \"\"\" # pip3 install lxml soup=BeautifulSoup(html_doc,'lxml') # 美化 # print(soup.prettify()) Tag 对象 from bs4.element import Tag #1、用法（通过.来查找，只能找到第一个） # head=soup.head # title=head.title # # print(head) # print(title) 获取标签名称 # p=soup.body # print(type(p)) # print(p.name) 获取标签属性 p=soup.p # 方式一 # 获取class属性,可以有多个，拿到列表 print(p['class']) print(p['id']) print(p.get('id')) # 方式二 print(p.attrs['class']) print(p.attrs.get('id')) 获取标签内容 p=soup.p print(p.text) # 所有层级都拿出来拼到一起 print(p.string) # 只有一层，才能去除 print(list(p.strings)) # 把每次都取出来，做成一个生成器 嵌套选择 title=soup.head.title print(title) 子节点 子孙节点 p1=soup.p.children # 迭代器 p2=soup.p.contents # 列表 print(list(p1)) print(p2) 父节点 祖先节点 p1=soup.p.parent # 直接父节点 p2=soup.p.parents print(p1) # print(len(list(p2))) print(list(p2)) 兄弟节点 print(soup.a.next_sibling) #下一个兄弟 print(soup.a.previous_sibling) #上一个兄弟 print(list(soup.a.next_siblings)) #下面的兄弟们=\u003e生成器对象 print(soup.a.previous_siblings) #上面的兄弟们=\u003e生成器对象 查找文档树 # 查找文档树（find，find_all），速度比遍历文档树慢 # 两个配合着使用（soup.p.find()） 五种过滤器(# 以find为例) 字符串 #1 字符串查找 引号内是字符串 p=soup.find(name='p') p=soup.find(name='body') print(p) # 查找类名是title的所有标签,class是关键字，class_ ret=soup.find_all(class_='title') href属性为http://example.com/elsie的标签 ret=soup.find_all(href='http://example.com/elsie') 找id为xx的标签 ret=soup.find_all(id='id_p') print(ret) 正则表达式 import re # reg=re.compile('^b') # ret=soup.find_all(name=reg) #找id以id开头的标签 reg=re.compile('^id') ret=soup.find_all(id=reg) print(ret) 列表 ret=soup.find_all(name=['body','b']) ret=soup.find_all(id=['id_p','link1']) ret=soup.find_all(class_=['id_p','link1']) # and 关系 ret=soup.find_all(class_='title',name='p') print(ret) True # 所有有名字的标签 ret=soup.find_all(name=True) #所有有id的标签 ret=soup.find_all(id=True) # 所有有herf属性的 ret=soup.find_all(href=True) print(ret) 方法 def has_class_but_no_id(tag): return tag.has_attr('class') and not tag.has_attr('id') print(soup.find_all(has_class_but_no_id)) 其他使用 ret=soup.find_all(attrs={'class':\"title\"}) ret=soup.find_all(attrs={'id':\"id_p1\",'class':'title'}) print(ret) 拿到标签取属性, 去text ret=soup.find_all(attrs={'id':\"id_p\",'class':'title'}) print(ret[0].text) limit(限制条数) soup.find() # 就是find_all limit=1 ret=soup.find_all(name=True,limit=2) print(len(ret)) recursive recursive=False (只找儿子)不递归查找，只找第一层 ret=soup.body.find_all(name='p',recursive=False) print(ret) ","date":"2017-07-04","objectID":"/posts/spider-2%E7%88%AC%E8%99%AB%E9%AB%98%E7%BA%A7/:0:1","tags":["爬虫","bs4"],"title":"爬虫入门/bs4","uri":"/posts/spider-2%E7%88%AC%E8%99%AB%E9%AB%98%E7%BA%A7/"},{"categories":["爬虫"],"content":"css 和 xpath 选择器 css 选择器 # 重点 # Tag对象.select(\"css选择器\") # #ID号 # .类名 # div\u003ep：儿子 和div p：子子孙孙 # 找div下最后一个a标签 div a:last-child # bs4：自己的选择器，css选择器 # lxml：css选择器，xpath选择器 # selenium：自己的选择器，css选择器，xpath选择器 # scrapy框架：自己的选择器，css选择器，xpath选择器 # #select('.article') #该模块提供了select方法来支持css,详见官网:https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html#id37 html_doc = \"\"\" \u003chtml\u003e\u003chead\u003e\u003ctitle\u003eThe Dormouse's story\u003c/title\u003e\u003c/head\u003e \u003cbody\u003e \u003cp class=\"title\"\u003e \u003cb\u003eThe Dormouse's story\u003c/b\u003e Once upon a time there were three little sisters; and their names were \u003ca href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"\u003e \u003cspan\u003eElsie\u003c/span\u003e \u003c/a\u003e \u003ca href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"\u003eLacie\u003c/a\u003e and \u003ca href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"\u003eTillie\u003c/a\u003e; \u003cdiv class='panel-1'\u003e \u003cul class='list' id='list-1'\u003e \u003cli class='element'\u003eFoo\u003c/li\u003e \u003cli class='element'\u003eBar\u003c/li\u003e \u003cli class='element'\u003eJay\u003c/li\u003e \u003c/ul\u003e \u003cul class='list list-small' id='list-2'\u003e \u003cli class='element'\u003e\u003ch1 class='yyyy'\u003eFoo\u003c/h1\u003e\u003c/li\u003e \u003cli class='element xxx'\u003eBar\u003c/li\u003e \u003cli class='element'\u003eJay\u003c/li\u003e \u003c/ul\u003e \u003c/div\u003e and they lived at the bottom of a well. \u003c/p\u003e \u003cp class=\"story\"\u003e...\u003c/p\u003e \"\"\" from bs4 import BeautifulSoup soup=BeautifulSoup(html_doc,'lxml') ### css 选择器 print(soup.p.select('.sister')) print(soup.select('.sister span')) print(soup.select('#link1')) print(soup.select('#link1 span')) print(soup.select('#list-2 .element.xxx')) print(soup.select('#list-2')[0].select('.element')) #可以一直select,但其实没必要,一条select就可以了 # xpath选择 # / 从根节点选取 /a 从根节点开始，往下找a标签（子） # //从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置 //a 从根节点开始找a标签（子子孙孙中所有a） # . 选取当前节点。 # .. 选取当前节点的父节点。 # @ 选取属性。 ######## # 2 xpath选择器 ######## # XPath 是一门在 XML 文档中查找信息的语言 # xpath选择 # / 从根节点选取 /a 从根节点开始，往下找a标签（子） # //从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置 //a 从根节点开始找a标签（子子孙孙中所有a） # 取值 /text() # 取属性 /@属性名 # //*[@id=\"auto-channel-lazyload-article\"]/ul[1] # //ul[1] # //*[@id=\"focus-1\"]/div[1]/ul/li[3]/h2 # #focus-1 \u003e div.focusimg-pic \u003e ul \u003e li:nth-child(3) \u003e h2 doc=''' \u003chtml\u003e \u003chead\u003e \u003cbase href='http://example.com/' /\u003e \u003ctitle\u003eExample website\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cdiv id='images'\u003e \u003ca href='image1.html' id=\"xxx\"\u003eName: My image 1 \u003cbr /\u003e\u003cimg src='image1_thumb.jpg' /\u003e\u003c/a\u003e \u003ch5\u003etest\u003c/h5\u003e \u003ca href='image2.html'\u003eName: My image 2 \u003cbr /\u003e\u003cimg src='image2_thumb.jpg' /\u003e\u003c/a\u003e \u003ca href='image3.html'\u003eName: My image 3 \u003cbr /\u003e\u003cimg src='image3_thumb.jpg' /\u003e\u003c/a\u003e \u003ca href='image4.html'\u003eName: My image 4 \u003cbr /\u003e\u003cimg src='image4_thumb.jpg' /\u003e\u003c/a\u003e \u003ca href='image5.html' class='li li-item' name='items'\u003eName: My image 5 \u003cbr /\u003e\u003cimg src='image5_thumb.jpg' /\u003e\u003c/a\u003e \u003ca href='image6.html' name='items'\u003e\u003cspan\u003e\u003ch5\u003etest\u003c/h5\u003e\u003c/span\u003eName: My image 6 \u003cbr /\u003e\u003cimg src='image6_thumb.jpg' /\u003e\u003c/a\u003e \u003c/div\u003e \u003c/body\u003e \u003c/html\u003e ''' from lxml import etree html=etree.HTML(doc) # 传字符串 # html=etree.parse('search.html',etree.HTMLParser()) # 文件 # 1 所有节点 # a=html.xpath('//*') # 2 指定节点（结果为列表） # a=html.xpath('//head') # 3 子节点，子孙节点 # a=html.xpath('//div/a') # a=html.xpath('//body/a') #无数据 # a=html.xpath('//body//a') # 4 父节点 # a=html.xpath('//body//a[@href=\"image1.html\"]/..') # a=html.xpath('//body//a[@href=\"image1.html\"]') # a=html.xpath('//body//a[1]/..') # 也可以这样 # a=html.xpath('//body//a[1]/parent::*') # 5 属性匹配 # a=html.xpath('//body//a[@href=\"image1.html\"]') # 6 文本获取 标签后加：/text() ********重点 # a=html.xpath('//body//a[@href=\"image1.html\"]/text()') # a=html.xpath('//body//a/text()') # 7 属性获取 标签后：/@href ********重点 # a=html.xpath('//body//a/@href') # # 注意从1 开始取（不是从0） # a=html.xpath('//body//a[3]/@href') # 8 属性多值匹配 # a 标签有多个class类，直接匹配就不可以了，需要用contains # a=html.xpath('//body//a[@class=\"li\"]') # a=html.xpath('//body//a[@href=\"image1.html\"]') # a=html.xpath('//body//a[contains(@class,\"li\")]') # a=html.xpath('//body//a[contains(@class,\"li\")]/text()') # a=html.xpath('//body//a[contains(@class,\"li\")]/@name') # 9 多属性匹配 or 和 and （了解） # a=html.xpath('//body//a[contains(@class,\"li\") or @name=\"items\"]') # a=html.xpath('//body//a[contains(@class,\"li\") and @name=\"items\"]/text()') # a=html.xpath('//body//a[contains(@class,\"li\"","date":"2017-07-04","objectID":"/posts/spider-2%E7%88%AC%E8%99%AB%E9%AB%98%E7%BA%A7/:0:2","tags":["爬虫","bs4"],"title":"爬虫入门/bs4","uri":"/posts/spider-2%E7%88%AC%E8%99%AB%E9%AB%98%E7%BA%A7/"},{"categories":null,"content":"爬虫 requests 模块使用 ","date":"2017-07-03","objectID":"/posts/spider-1%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/:0:0","tags":["爬虫","requests模块"],"title":"Request模块","uri":"/posts/spider-1%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/"},{"categories":null,"content":"爬虫介绍 # 1 本质：模拟发送http请求（requests）----》解析返回数据（re，bs4，lxml，json）---》入库（redis，mysql，mongodb） # 2 app爬虫：本质一模一样 # 3 为什么python做爬虫最好：包多，爬虫框架：scrapy：性能很高的爬虫框架，爬虫界的django，大而全（爬虫相关的东西都集成了） # 4 百度，谷歌，就是个大爬虫 在百度搜索，其实是去百度的服务器的库搜的，百度一直开着爬虫，一刻不停的在互联网上爬取，把页面存储到自己库中 # 5 全文检索：全文检索 ","date":"2017-07-03","objectID":"/posts/spider-1%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/:0:1","tags":["爬虫","requests模块"],"title":"Request模块","uri":"/posts/spider-1%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/"},{"categories":null,"content":"requests 模块 requests模块是基于urllib2 内置库的基础上封装的一个模块, 被广泛应用 安装方法 pip3 install requests 基本使用 返回值的属性 import requests # # 发送http请求 # # get,delete,post。。本质都是调用request函数 # ret=requests.get('https://www.cnblogs.com') # print(ret.status_code) # 响应状态码 # print(ret.text) # 响应体，转成了字符串 # print(ret.content) # 响应体，二进制 # ret=requests.post()\\ # ret=requests.request(\"get\",) # ret=requests.delete() ## get 请求 带参数 get请求带参数 方式一 ret = requests.get('https://www.baidu.com/', headers={ 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36', }) 方式二（建议用方式二）中文会自动转码 ret=requests.get('http://0.0.0.0:8001/',params={'name':\"美女\",'age':18}) print(ret.text) 带 headers ret = requests.get('http://0.0.0.0:8001/?name=%E7%BE%8E%E5%A5%B3', headers={ # 标志，什么东西发出的请求，浏览器信息，django框架，从哪取？（meta） 'User-Agent': 'request', # 上一个页面的地址，图片防盗链 'Referer': 'xxx' }) print(ret) # 图片防盗链：如果图片的referer不是我自己的网站，就直接禁止掉 \u003cimg src=\"https://www.lgstatic.com/lg-community-fed/community/modules/common/img/avatar_default_7225407.png\"\u003e 带 cookie # 带cookie,随机字符串(用户信息：也代表session)，不管后台用的token认证，还是session认证 # 一旦登陆了，带着cookie发送请求，表示登陆了（下单，12306买票，评论） 方式一 ret = requests.get('http://0.0.0.0:8001/?name=%E7%BE%8E%E5%A5%B3', headers={ 'cookie': 'key3=value;key2=value', }) 方式二 ret = requests.get('http://0.0.0.0:8001/?name=%E7%BE%8E%E5%A5%B3', cookies={\"islogin\":\"xxx\"}) print(ret) 发送post 请求 5 发送post请求（注册，登陆），携带数据（body） data=None, json=None # data:urlencoded编码 ret=requests.post('http://0.0.0.0:8001/',data={'name':\"lqz\",'age':18}) # json:json编码 import json data=json.dumps({'name':\"lqz\",'age':18}) ret=requests.post('http://0.0.0.0:8001/',json=data) print(ret) # 注意：编码格式是请求头中带的，所有我可以手动修改，在headers中改 session 对象 session=requests.session() # 跟requests.get/post用起来完全一样，但是它处理了cookie # 假设是一个登陆，并且成功 session.post() # 再向该网站发请求，就是登陆状态，不需要手动携带cookie session.get(\"地址\") 响应对象 print(respone.text) # 响应体转成str print(respone.content) # 响应体二进制（图片，视频） print(respone.status_code) # 响应状态码 print(respone.headers) # 响应头 print(respone.cookies) # 服务端返回的cookie print(respone.cookies.get_dict()) # 转成字典 print(respone.cookies.items()) print(respone.url) # 当次请求的地址 print(respone.history) # 如果有重定向，放到一个列表中 ret=requests.post('http://0.0.0.0:8001/') ret=requests.get('http://0.0.0.0:8001/admin') #不要误解 ret=requests.get('http://0.0.0.0:8001/user') print(ret.history) print(respone.encoding) # 编码方式 response.iter_content() # 视频，图片迭代取值 with open(\"a.mp4\",'wb') as f: for line in response.iter_content(): f.write(lin 乱码问题 # 加载回来的页面，打印出来，乱码（我们用的是utf8编码），如果网站用gbk， ret.encoding='gbk' ret=requests.get('http://0.0.0.0:8001/user') # ret.apparent_encoding当前页面的编码 ret.encoding=ret.apparent_encoding 解析json # 返回数据，有可能是json格式，有可能是html格式 ret=requests.get('http://0.0.0.0:8001/') print(type(ret.text)) print(ret.text) # a=ret.json() print(a['name']) print(type(a)) 使用代理 proxies={} # 正向代理 # django如何拿到客户端ip地址 META.get(\"REMOTE_ADDR\") # 如何去获取代理，如何使用（用自己项目验收） # 使用代理有什么用 ret=requests.get('http://0.0.0.0:8001/',proxies={'http':'地址'}) print(type(ret.text)) print(ret.text) 异常处理 # 用try except捕获一下 就用它就型了：Exception 上传文件 (爬虫用的比较少，后台写服务，) file={'myfile':open(\"1.txt\",'rb')} ret=requests.post('http://0.0.0.0:8001/',files=file) print(ret.content) ","date":"2017-07-03","objectID":"/posts/spider-1%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/:0:2","tags":["爬虫","requests模块"],"title":"Request模块","uri":"/posts/spider-1%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/"},{"categories":null,"content":"爬梨视频 ############ # 2 爬取视频 ############# #categoryId=9 分类id #start=0 从哪个位置开始，每次加载12个 # https://www.pearvideo.com/category_loading.jsp?reqType=5\u0026categoryId=9\u0026start=0 import requests import re ret=requests.get('https://www.pearvideo.com/category_loading.jsp?reqType=5\u0026categoryId=9\u0026start=0') # print(ret.text) # 正则取解析 reg='\u003ca href=\"(.*?)\" class=\"vervideo-lilink actplay\"\u003e' video_urls=re.findall(reg,ret.text) print(video_urls) for url in video_urls: ret_detail=requests.get('https://www.pearvideo.com/'+url) reg='srcUrl=\"(.*?)\",vdoUrl=srcUrl' mp4_url=re.findall(reg,ret_detail.text)[0] #type:str # 下载视频 video_content=requests.get(mp4_url) video_name=mp4_url.rsplit('/',1)[-1] with open(video_name,'wb') as f: for line in video_content.iter_content(): f.write(line) ","date":"2017-07-03","objectID":"/posts/spider-1%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/:0:3","tags":["爬虫","requests模块"],"title":"Request模块","uri":"/posts/spider-1%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/"},{"categories":null,"content":"模拟登陆 ############ # 3 模拟登陆某网站 ############# import requests ret = requests.post('http://www.aa7a.cn/user.php', data={ 'username': '616564099@qq.com', 'password': 'lqz123', 'captcha': 'f5jn', 'remember': '1', 'ref': 'http://www.aa7a.cn/', 'act': 'act_login', }) cookie=ret.cookies.get_dict() print(cookie) # 如果不出意外，咱么就登陆上了,再向首页发请求，首页返回的数据中就有616564099@qq.com ret1=requests.get('http://www.aa7a.cn/',cookies=cookie) # ret1=requests.get('http://www.aa7a.cn/') print('616564099@qq.com' in ret1.text) # 秒杀小米手机，一堆小号 # 定时任务：一到时间，就可以发送post请求，秒杀手机 # 以后碰到特别难登陆的网站，代码登陆不进去怎么办？ # 之所以要登陆，就是为了拿到cookie，下次发请求（如果程序拿不到cookie，自动登陆不进去） # 就手动登陆进去，然后用程序发请求 ","date":"2017-07-03","objectID":"/posts/spider-1%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/:0:4","tags":["爬虫","requests模块"],"title":"Request模块","uri":"/posts/spider-1%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/"},{"categories":["异步"],"content":"celery 的简单使用 ","date":"2017-06-03","objectID":"/posts/celery/:0:0","tags":["celery","FixIt"],"title":"Celery","uri":"/posts/celery/"},{"categories":["异步"],"content":"官方 Celery 官网：http://www.celeryproject.org/ Celery 官方文档英文版：http://docs.celeryproject.org/en/latest/index.html Celery 官方文档中文版：http://docs.jinkan.org/docs/celery/ ","date":"2017-06-03","objectID":"/posts/celery/:1:0","tags":["celery","FixIt"],"title":"Celery","uri":"/posts/celery/"},{"categories":["异步"],"content":"Celery异步任务框架 \"\"\" 1）可以不依赖任何服务器，通过自身命令，启动服务(内部支持socket) 2）celery服务为为其他项目服务提供异步解决任务需求的 注：会有两个服务同时运行，一个是项目服务，一个是celery服务，项目服务将需要异步处理的任务交给celery服务，celery就会在需要时异步完成项目的需求 人是一个独立运行的服务 | 医院也是一个独立运行的服务 正常情况下，人可以完成所有健康情况的动作，不需要医院的参与；但当人生病时，就会被医院接收，解决人生病问题 人生病的处理方案交给医院来解决，所有人不生病时，医院独立运行，人生病时，医院就来解决人生病的需求 \"\"\" ","date":"2017-06-03","objectID":"/posts/celery/:2:0","tags":["celery","FixIt"],"title":"Celery","uri":"/posts/celery/"},{"categories":["异步"],"content":"Celery架构 Celery的架构由三部分组成，消息中间件（message broker）、任务执行单元（worker）和 任务执行结果存储（task result store）组成。 ","date":"2017-06-03","objectID":"/posts/celery/:3:0","tags":["celery","FixIt"],"title":"Celery","uri":"/posts/celery/"},{"categories":["异步"],"content":"消息中间件 Celery本身不提供消息服务，但是可以方便的和第三方提供的消息中间件集成。包括，RabbitMQ, Redis等等 ","date":"2017-06-03","objectID":"/posts/celery/:3:1","tags":["celery","FixIt"],"title":"Celery","uri":"/posts/celery/"},{"categories":["异步"],"content":"任务执行单元 Worker是Celery提供的任务执行的单元，worker并发的运行在分布式的系统节点中。 ","date":"2017-06-03","objectID":"/posts/celery/:3:2","tags":["celery","FixIt"],"title":"Celery","uri":"/posts/celery/"},{"categories":["异步"],"content":"任务结果存储 Task result store用来存储Worker执行的任务的结果，Celery支持以不同方式存储任务的结果，包括AMQP, redis等 ","date":"2017-06-03","objectID":"/posts/celery/:3:3","tags":["celery","FixIt"],"title":"Celery","uri":"/posts/celery/"},{"categories":["异步"],"content":"使用场景 异步执行：解决耗时任务 延迟执行：解决延迟任务 定时执行：解决周期(周期)任务 ","date":"2017-06-03","objectID":"/posts/celery/:3:4","tags":["celery","FixIt"],"title":"Celery","uri":"/posts/celery/"},{"categories":["异步"],"content":"Celery的安装配置 pip install celery 消息中间件：RabbitMQ/Redis app=Celery(‘任务名’, broker=‘xxx’, backend=‘xxx’) ","date":"2017-06-03","objectID":"/posts/celery/:4:0","tags":["celery","FixIt"],"title":"Celery","uri":"/posts/celery/"},{"categories":["异步"],"content":"两种celery任务结构：提倡用包管理，结构更清晰 # 如果 Celery对象:Celery(...) 是放在一个模块下的 # 1）终端切换到该模块所在文件夹位置：scripts # 2）执行启动worker的命令：celery worker -A 模块名 -l info -P eventlet # 注：windows系统需要eventlet支持，Linux与MacOS直接执行：celery worker -A 模块名 -l info # 注：模块名随意 # 如果 Celery对象:Celery(...) 是放在一个包下的 # 1）必须在这个包下建一个celery.py的文件，将Celery(...)产生对象的语句放在该文件中 # 2）执行启动worker的命令：celery worker -A 包名 -l info -P eventlet # 注：windows系统需要eventlet支持，Linux与MacOS直接执行：celery worker -A 模块名 -l info # 注：包名随意 ","date":"2017-06-03","objectID":"/posts/celery/:4:1","tags":["celery","FixIt"],"title":"Celery","uri":"/posts/celery/"},{"categories":["异步"],"content":"Celery执行异步任务 ","date":"2017-06-03","objectID":"/posts/celery/:5:0","tags":["celery","FixIt"],"title":"Celery","uri":"/posts/celery/"},{"categories":["异步"],"content":"包架构封装 project ├── celery_task # celery包 │ ├── __init__.py # 包文件 │ ├── celery.py # celery连接和配置相关文件，且名字必须交celery.py │ └── tasks.py # 所有任务函数 ├── add_task.py # 添加任务 └── get_result.py # 获取结果 ","date":"2017-06-03","objectID":"/posts/celery/:5:1","tags":["celery","FixIt"],"title":"Celery","uri":"/posts/celery/"},{"categories":["异步"],"content":"基本使用 celery.py # 1）创建app + 任务 # 2）启动celery(app)服务： # 非windows # 命令：celery worker -A celery_task -l info # windows： # pip3 install eventlet # celery worker -A celery_task -l info -P eventlet # 3）添加任务：手动添加，要自定义添加任务的脚本，右键执行脚本 # 4）获取结果：手动获取，要自定义获取任务的脚本，右键执行脚本 from celery import Celery broker = 'redis://127.0.0.1:6379/1' backend = 'redis://127.0.0.1:6379/2' app = Celery(broker=broker, backend=backend, include=['celery_task.tasks']) tasks.py from .celery import app import time @app.task def add(n, m): print(n) print(m) time.sleep(10) print('n+m的结果：%s' % (n + m)) return n + m @app.task def low(n, m): print(n) print(m) print('n-m的结果：%s' % (n - m)) return n - m add_task.py from celery_task import tasks # 添加立即执行任务 t1 = tasks.add.delay(10, 20) t2 = tasks.low.delay(100, 50) print(t1.id) # 添加延迟任务 from datetime import datetime, timedelta eta=datetime.utcnow() + timedelta(seconds=10) tasks.low.apply_async(args=(200, 50), eta=eta) get_result.py from celery_task.celery import app from celery.result import AsyncResult id = '21325a40-9d32-44b5-a701-9a31cc3c74b5' if __name__ == '__main__': async = AsyncResult(id=id, app=app) if async.successful(): result = async.get() print(result) elif async.failed(): print('任务失败') elif async.status == 'PENDING': print('任务等待中被执行') elif async.status == 'RETRY': print('任务异常后正在重试') elif async.status == 'STARTED': print('任务已经开始被执行') ","date":"2017-06-03","objectID":"/posts/celery/:5:2","tags":["celery","FixIt"],"title":"Celery","uri":"/posts/celery/"},{"categories":["异步"],"content":"高级使用 celery.py # 1）创建app + 任务 # 2）启动celery(app)服务： # 非windows # 命令：celery worker -A celery_task -l info # windows： # pip3 install eventlet # celery worker -A celery_task -l info -P eventlet # 3）添加任务：自动添加任务，所以要启动一个添加任务的服务 # 命令：celery beat -A celery_task -l info # 4）获取结果 from celery import Celery broker = 'redis://127.0.0.1:6379/1' backend = 'redis://127.0.0.1:6379/2' app = Celery(broker=broker, backend=backend, include=['celery_task.tasks']) # 时区 app.conf.timezone = 'Asia/Shanghai' # 是否使用UTC app.conf.enable_utc = False # 任务的定时配置 from datetime import timedelta from celery.schedules import crontab app.conf.beat_schedule = { 'low-task': { 'task': 'celery_task.tasks.low', 'schedule': timedelta(seconds=3), # 'schedule': crontab(hour=8, day_of_week=1), # 每周一早八点 'args': (300, 150), } } tasks.py from .celery import app import time @app.task def add(n, m): print(n) print(m) time.sleep(10) print('n+m的结果：%s' % (n + m)) return n + m @app.task def low(n, m): print(n) print(m) print('n-m的结果：%s' % (n - m)) return n - m get_result.py from celery_task.celery import app from celery.result import AsyncResult id = '21325a40-9d32-44b5-a701-9a31cc3c74b5' if __name__ == '__main__': async = AsyncResult(id=id, app=app) if async.successful(): result = async.get() print(result) elif async.failed(): print('任务失败') elif async.status == 'PENDING': print('任务等待中被执行') elif async.status == 'RETRY': print('任务异常后正在重试') elif async.status == 'STARTED': print('任务已经开始被执行') ","date":"2017-06-03","objectID":"/posts/celery/:5:3","tags":["celery","FixIt"],"title":"Celery","uri":"/posts/celery/"},{"categories":["异步"],"content":"django中使用 celery.py \"\"\" celery框架django项目工作流程 1）加载django配置环境 2）创建Celery框架对象app，配置broker和backend，得到的app就是worker 3）给worker对应的app添加可处理的任务函数，用include配置给worker的app 4）完成提供的任务的定时配置app.conf.beat_schedule 5）启动celery服务，运行worker，执行任务 6）启动beat服务，运行beat，添加任务 重点：由于采用了django的反射机制，使用celery.py所在的celery_task包必须放置项目的根目录下 \"\"\" # 一、加载django配置环境 import os os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"luffyapi.settings.dev\") # 二、加载celery配置环境 from celery import Celery # broker broker = 'redis://127.0.0.1:6379/0' # backend backend = 'redis://127.0.0.1:6379/1' # worker app = Celery(broker=broker, backend=backend, include=['celery_task.tasks']) # 时区 app.conf.timezone = 'Asia/Shanghai' # 是否使用UTC app.conf.enable_utc = False # 任务的定时配置 from datetime import timedelta from celery.schedules import crontab app.conf.beat_schedule = { 'update-banner-list': { 'task': 'celery_task.tasks.update_banner_list', 'schedule': timedelta(seconds=10), 'args': (), } } tasks.py from .celery import app from django.core.cache import cache from home import models, serializers from django.conf import settings @app.task def update_banner_list(): queryset = models.Banner.objects.filter(is_delete=False, is_show=True).order_by('-orders')[:settings.BANNER_COUNT] banner_list = serializers.BannerSerializer(queryset, many=True).data # 拿不到request对象，所以头像的连接base_url要自己组装 for banner in banner_list: banner['image'] = 'http://127.0.0.1:8000%s' % banner['image'] cache.set('banner_list', banner_list, 86400) return True ","date":"2017-06-03","objectID":"/posts/celery/:5:4","tags":["celery","FixIt"],"title":"Celery","uri":"/posts/celery/"}]